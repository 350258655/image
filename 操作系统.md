#### 操作系统结构

****

##### 什么是内核

​&ensp;&ensp;计算机是由各种外部硬件设备组成的，比如内存、cpu、硬盘等，如果每个应用都要和这些硬件设备对接通信协议，那这样太累了，所以这个中间人就由内核来负责，让内核作为应用连接硬件设备的桥梁，应用程序只需关心与内核交互，不用关心硬件的细节。

****

##### 内核有哪些功能？

&ensp;&ensp;现代操作系统，内核一般会提供 4 个基本能力：

- 管理进程、线程，决定哪个进程、线程使用 CPU，也就是进程调度的能力
- 管理内存，决定内存的分配和回收，也就是内存管理的能力
- 管理硬件设备，为进程与硬件设备之间提供通信能力，也就是硬件通信能力
- 提供系统调用，如果应用程序要运行更高权限的服务，那么就需要有系统调用，它是用户程序与操作系统之间的接口。

****

##### 内核是怎么工作的？

​&ensp;&ensp;内核具有很高的权限，可以控制 cpu、内存、硬盘等硬件，而应用程序具有的权限很小，因此大多数操作系统，把内存分成了两个区域：

- 内核空间，这个内存空间只有内核程序可以访问；
- 用户空间，这个内存空间专门给应用程序使用；

​&ensp;&ensp;用户空间的代码只能访问一个局部的内存空间，而内核空间的代码可以访问所有内存空间。因此，当程序使用用户空间时，我们常说该程序在用户态执行，而当程序使内核空间时，程序则在内核态执行。应用程序如果需要进入内核空间，就需要通过系统调用。

​&ensp;&ensp;内核程序运行在内核态，用户程序运行在用户态。当应用程序使用系统调用时，会产生一个中断。发生中断后， CPU 会中断当前在执⾏的用户程序，转⽽跳转到中断处理程序，也就是开始执行内核程序。内核处理完后，主动触发中断，把 CPU 执行权限交回给用户程序，回到用户态继续工作。

****



#### 内存管理

****

##### 什么是内存分段？

​&ensp;&ensp;程序是由若干个逻辑分段组成的，如可由代码段、数据段、栈段、堆段组成。不同的段是有不同的属性的，所以就用分段的形式把这些段分离出来。

****

##### 分段机制下，虚拟地址和物理地址是如何映射的？

​	分段机制下的虚拟地址由两部分组成，**段选择子**和**段内偏移量**。

![avrter]([OS]内存分段示例.png)

- **段选择子**就保存在段寄存器里面。段选择子里面最重要的是段号，用作段表的索引。段表项里面保存的 是这个段的基地址、段的界限和特权等级等。
- 虚拟地址中的段内偏移量应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段 内偏移量得到物理内存地址。

​    在上面，知道了虚拟地址是通过段表与物理地址进行映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段在段表中有一个项，在这一项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址，如下图：

![avr]([OS]内存分段举例.png)

​    如果要访问段 3 中偏移量 500 的虚拟地址，我们可以计算出物理地址为，段 3 基地址 7000 + 偏移量 500= 7500。

****

##### 分段为什么会产生内存碎⽚的问题？

​    我们来看看这样一个例⼦。假设有 1G 的物理内存，用户执⾏了多个程序，其中：

- 游戏占用了 512MB 内存
- 浏览器占用了 128MB 内存
- 音乐占用了 256 MB 内存

​    这个时候，如果我们关闭了浏览器，则空闲内存还有 1024 - 512 - 256 = 256MB。如果这个 256MB 不是连续的，被分成了两段 128 MB 内存，这就会导致没有空间再打开一个 200MB 的程序。

​    解决这种内存碎⽚的方法就是**内存交换**。

​    可以把音乐程序占用的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存里。不过再读回的时候，我们不能装载回原来的位置，⽽是紧紧跟着那已经被占用了的 512MB 内存后⾯。这样就能空缺出连续的 256MB 空间，于是新的 200MB 程序就可以装载进来。
​    这个内存交换空间，在 Linux 系统里，也就是我们常看到的 Swap 空间，这块空间是从硬盘划分出来的，用于内存与硬盘的空间交换。

****

##### 分段为什么会导致内存交换效率低的问题？

​    对于多进程的系统来说，用分段的⽅式，内存碎⽚是很容易产生的，产生了内存碎⽚，那不得不重新 Swap 内存区域，这个过程会产生性能瓶颈。 

​     因为硬盘的访问速度要⽐内存慢太多了，每一次内存交换，我们都需要把一⼤段连续的内存数据写到硬盘 上。 所以，**如果内存交换的时候，交换的是一个占内存空间很⼤的程序，这样整个机器都会显得卡顿**。 

​    为了解决内存分段的内存碎⽚和内存交换效率低的问题，就出现了内存分⻚。

****

##### 说说你对内存分页机制的了解?

​	分⻚是把**整个虚拟和物理内存空间切成一段段固定尺⼨的⼤⼩**。这样一个连续并且尺⼨固定的内存空间，我们叫页（Page）。在 Linux 下，每一⻚的⼤⼩为 4KB 。虚拟地址与物理地址之间通过页表来映射，如下图：

![a]([OS]内存分页示例.png)

​	页表是存储在内存(每个进程都拥有自己的页表)里的，内存管理单元 （MMU）就做将虚拟内存地址转换成物理地址的⼯作。

​	⽽当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常，进⼊系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运⾏。

****

##### 分页是怎么解决分段的内存碎片、内存交换效率低的问题？

​	使用分页能解决内存碎片问题，主要原因是：

-  内存空间都是预先划分好的，也就不会像分段会产生间隙非常小的内存
- 采用了分页，那么释放的内存都是以页为单位释放的，也就不会产生⽆法给进程使用的小内存

​	使用分页能解决内存交换效率低的问题，主要是因为一次性写入磁盘的只有少数一个页或者几个页，不会花太多时间。

​	更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。

​	我们在进行虚拟内存和物理内存的页之间的映射之后，并不真的把页加载到物理内存里，而是只有在程序运⾏中，需要用到对应虚拟内存页里⾯的指令和数据时，再加载到物理内存里⾯去。

***

##### 分⻚机制下，虚拟地址和物理地址是如何映射的？

​	在分⻚机制下，虚拟地址分为两部分，**⻚号**和**⻚内偏移**。⻚号作为⻚表的索引，⻚表包含物理⻚每⻚所在物理内存的基地址，这个基地址与⻚内偏移的组合就形成了物理内存地址，⻅下图。

<img src="[OS]虚存页表映射.png" alt="avg" style="zoom:50%;" />

​	总结⼀下，对于⼀个内存地址转换，其实就是这样三个步骤(所以就是根据虚拟页号，找到物理页号)：

- 把虚拟内存地址，切分成⻚号和偏移量；
- 根据⻚号，从⻚表⾥⾯，查询对应的物理⻚号；
- 直接拿物理⻚号，加上前⾯的偏移量，就得到了物理内存地址。

***

##### 简单的分⻚有什么缺陷吗？

​	有空间上的缺陷。

​	在 32 位的环境下，虚拟地址空间共有 4GB，假设⼀个⻚的⼤⼩是 4KB（2^12），那么就需要 2^20 个⻚，每个「⻚表项」需要 4 个字节⼤⼩来存储，那么整个 4GB 空间的映射就需要有 4MB的内存来存储⻚表。

​	这 4MB ⼤⼩的⻚表，看起来也不是很⼤。但是要知道每个进程都是有⾃⼰的虚拟地址空间的，也就说都有⾃⼰的⻚表。那么，100 个进程的话，就需要 400MB 的内存来存储⻚表，这是⾮常⼤的内存了，更别说 64 位的环境了。

***

##### 多级⻚表的结构？

​	我们把单级页表中 2^20 个「⻚表项」的单级⻚表再分⻚，将⻚表（⼀级⻚表）分为 1024 个⻚表（⼆级⻚表, 也就是2^10），每个表（⼆级⻚表）中包含 1024 个「⻚表项」，形成⼆级分⻚。如下图所示：

<img src="[OS]多级页表.png" alt="avg" style="zoom:50%;" />

***

##### 分⼆级表后，映射 4G 地址空间就需要 4K(⼀级⻚表)+ 4M(⼆级⻚表)的内存，占用空间更大了？
​	如果 4GB 的虚拟地址全部都映射到了物理内存上的话，⼆级分⻚占⽤空间确实是更⼤了，但是，我们往往不会为⼀个进程分配那么多内存。

​	如果使⽤了⼆级分⻚，⼀级⻚表就可以覆盖整个 4GB 虚拟地址空间，但如果某个⼀级⻚表的⻚表项没有被⽤到，也就不需要创建这个⻚表项对应的⼆级⻚表了，即可以在需要时才创建⼆级⻚表。做个简单的计算，假设只有 20% 的⼀级⻚表项被⽤到了，那么⻚表占⽤的内存空间就只有 4KB（⼀级⻚表） + 20% *4MB（⼆级⻚表）= 0.804MB 。

*****

##### TLB的作用？

​	首先说说为什么需要TLB：因为页表是在内存中的，如果我要访问一个内存地址，还需要到内存中访问页表，这就降低了访问速度。

​	所以我们可以加入一个缓存层，把最常访问的页表项存储到访问速度更快的硬件。这就是TLB，也称为快表。

​	由于程序局部性原理的存在，使得TLB的命中率非常高

*****

##### 段⻚式内存管理的实现

​	段⻚式内存管理实现的⽅式：

- 先将程序划分为多个有逻辑意义的段，也就是前⾯提到的分段机制；
- 接着再把每个段划分为多个⻚，也就是对分段划分出来的连续空间，再划分固定⼤⼩的⻚；

​    这样，地址结构就由**段号**、**段内⻚号**和**⻚内位移**三部分组成。

​	⽤于段⻚式地址变换的数据结构是每⼀个程序⼀张段表，每个段⼜建⽴⼀张⻚表，段表中的地址是⻚表的起始地址，⽽⻚表中的地址则为某⻚的物理⻚号，如图所示：

<img src="[OS]段页式内存管理.png" alt="avg" style="zoom:50%;">

​	段⻚式地址变换中要得到物理地址须经过三次内存访问：

- 第⼀次访问段表，得到⻚表起始地址；
- 第⼆次访问⻚表，得到物理⻚号；
- 第三次将物理⻚号与⻚内位移组合，得到物理地址。

***

##### intel 80386 是如何进行内存管理的？

​	80386 采用页式管理内存，但它的设计没有绕开段式内存管理，⽽是建⽴在段式内存管理的基础上，这就意味着，⻚式内存管理的作⽤是在由段式内存管理所映射⽽成的地址上再加上⼀层地址映射。

​	由于此时由段式内存管理映射⽽成的地址不再是“物理地址”了，Intel 就称之为“线性地址”（也称虚拟地址）。于是，段式内存管理先将**逻辑地址**映射成**线性地址**，然后再由⻚式内存管理将**线性地址**映射成**物理地址**。

​	逻辑地址是「段式内存管理」转换前的地址，线性地址则是「⻚式内存管理」转换前的地址。

****

##### 那么Linux 采⽤了什么⽅式管理内存？

​	**Linux 内存主要采⽤的是⻚式内存管理，但同时也不可避免地涉及了段机制。**

​	这主要是上⾯ Intel 处理器发展历史导致的，因为 Intel X86 CPU ⼀律对程序中使⽤的地址先进⾏段式映射，然后才能进⾏⻚式映射。既然 CPU 的硬件结构是这样，Linux 内核也只好服从 Intel 的选择。但是事实上，Linux 内核所采取的办法是使段式映射的过程实际上不起什么作⽤。

​	*Linux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下），也就是所有的段的起始地址都是⼀样的。这意味着，Linux系统中的代码，包括操作系统本身的代码和应⽤程序代码，所⾯对的地址空间都是线性地址空间（虚拟地址），这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被⽤于访问控制和内存保护。*

****

##### 页表项的结构

​	⻚表项通常有如下图的字段：

<img src="[OS]页表项结构.png" alt="avg" style="zoom:50%;">

- 状态位：⽤于表示该⻚是否有效，也就是说是否在物理内存中，供程序访问时参考。
- 访问字段：⽤于记录该⻚在⼀段时间被访问的次数，供⻚⾯置换算法选择出⻚⾯时参考。
- 修改位：表示该⻚在调⼊内存后是否有被修改过，由于内存中的每⼀⻚都在磁盘上保留⼀份副本，因此，如果没有修改，在置换该⻚时就不需要将该⻚写回到磁盘上，以减少系统的开销；如果已经被修改，则将该⻚重写到磁盘上，以保证磁盘中所保留的始终是最新的副本。
- 硬盘地址：⽤于指出该⻚在硬盘上的地址，通常是物理块号，供调⼊该⻚时使⽤。

***

##### 页面置换算法有哪些？

​	常⻅的⻚⾯置换算法有如下⼏种:

- 最佳⻚⾯置换算法（OPT）
- 先进先出置换算法（FIFO
- 最近最久未使⽤的置换算法（LRU）
- 时钟⻚⾯置换算法（Lock）
- 最不常⽤置换算法（LFU）

 1、最佳页面置换算法

​	最佳⻚⾯置换算法基本思路是，置换在「未来」最⻓时间不访问的⻚⾯。但因为程序访问⻚⾯时是动态的，我们是⽆法预知每个⻚⾯在「下⼀次」访问前的等待时间。所以，最佳⻚⾯置换算法作⽤是为了衡量你的算法的效率，你的算法效率越接近该算法的效率，那么说明你的算法是⾼效的。

2、先进先出置换算法

​	选择在内存驻留时间很⻓的⻚⾯进⾏中置换

3、最近最久未使⽤的置换算法

​	最近最久未使⽤（LRU）的置换算法的基本思路是，发⽣缺⻚时，选择最⻓时间没有被访问的⻚⾯进⾏置换，也就是说，该算法假设已经很久没有使⽤的⻚⾯很有可能在未来较⻓的⼀段时间内仍然不会被使⽤。

​	虽然 LRU 在理论上是可以实现的，但代价很⾼。为了完全实现 LRU，需要在内存中维护⼀个所有⻚⾯的链表，最近最多使⽤的⻚⾯在表头，最近最少使⽤的⻚⾯在表尾。

​	困难的是，在每次访问内存时都必须要更新「整个链表」。在链表中找到⼀个⻚⾯，删除它，然后把它移动到表头是⼀个⾮常费时的操作。

​	所以，LRU 虽然看上去不错，但是由于开销⽐较⼤，实际应⽤中⽐较少使⽤。

4、时钟⻚⾯置换算法

​	时钟⻚⾯置换算法的思路是，把所有的⻚⾯都保存在⼀个类似钟⾯的「环形链表」中，⼀个表针指向最⽼的⻚⾯。当发⽣缺⻚中断时，算法⾸先检查表针指向的⻚⾯：

- 如果它的访问位位是 0 就淘汰该⻚⾯，并把新的⻚⾯插⼊这个位置，然后把表针前移⼀个位置；
- 如果访问位是 1 就清除访问位，并把表针前移⼀个位置，重复这个过程直到找到了⼀个访问位为 0 的⻚⾯为⽌；

5、最不常⽤算法(LFU)

​	当发⽣缺⻚中断时，选择「访问次数」最少的那个⻚⾯，并将其淘汰。

​	它的实现⽅式是，对每个⻚⾯设置⼀个「访问计数器」，每当⼀个⻚⾯被访问时，该⻚⾯的访问计数器就累加 1。在发⽣缺⻚中断时，淘汰计数器值最⼩的那个⻚⾯

​	要增加⼀个计数器来实现，这个硬件成本是⽐较⾼的，另外如果要对这个计数器查找哪个⻚⾯访问次数最⼩，查找链表本身，如果链表⻓度很⼤，是⾮常耗时的，效率不⾼。

***



#### 进程和线程

****

##### 简单说下你对并发和并行的理解？

​	并行是指两个或者多个事件在同一时刻发生；而并发是指两个或多个事件在同一时间间隔发生

****

##### 进程和线程的基本概念

- 进程：进程是系统进行资源分配和调度的一个独立单位，是系统中的并发执行的单位。
- 线程：线程是进程当中的⼀条执⾏流程。同⼀个进程内多个线程之间可以共享代码段、数据段、打开的⽂件等资源，但每个线程各⾃都有⼀套独⽴的寄存器和栈，这样可以确保线程的控制流是相对独⽴的。

****

##### 进程与线程的区别？

- 进程是资源分配的最小单位，而线程是 CPU 调度的最小单位；
- 创建进程或撤销进程，系统都要为之分配或回收资源，操作系统开销远大于创建或撤销线程时的开销
- 不同进程地址空间相互独立，同一进程内的线程共享同一地址空间。一个进程的线程在另一个进程内是不可见的；
- 进程间不会相互影响，而一个线程挂掉将可能导致整个进程挂掉；

****

##### 为什么有了进程，还要有线程呢？

​	进程可以使多个程序并发执行，以提高资源的利用率和系统的吞吐量，但是其带来了一些缺点：

- 进程在同一时间只能干一件事情
- 进程在执行的过程中如果阻塞，整个进程就会被挂起，即使进程中有些工作不依赖于等待的资源，仍然不会执行

​	基于以上的缺点，操作系统引入了比进程粒度更小的线程，作为并发执行的基本单位，从而减少程序在并发执行时所付出的时间和空间开销，提高并发性能。

****

##### 进程的五态模型

- 运行态：运行态指的就是进程实际占用 CPU 时间片运行时
- 就绪态：就绪态指的是可运行，但因为其他进程正在运行而处于就绪状态
- 阻塞态：阻塞态又被称为睡眠态，它指的是进程不具备运行条件，正在等待被 CPU 调度
- 新建态：进程的新建态就是进程刚创建出来的时候
- 终止态：进程的终止态就是指进程执行完毕，到达结束点，或者因为错误而不得不中止进程

****

##### 进程的状态变换

- NULL -> 创建状态：⼀个新进程被创建时的第⼀个状态；
- 创建状态 -> 就绪状态：当进程被创建完成并初始化后，⼀切就绪准备运⾏时，变为就绪状态，这个过程是很快的；
- 就绪态 -> 运⾏状态：处于就绪状态的进程被操作系统的进程调度器选中后，就分配给 CPU 正式运⾏该进程；
- 运⾏状态 -> 结束状态：当进程已经运⾏完成或出错时，会被操作系统作结束状态处理；
- 运⾏状态 -> 就绪状态：处于运⾏状态的进程在运⾏过程中，由于分配给它的运⾏时间⽚⽤完，操作系统会把该进程变为就绪态，接着从就绪态选中另外⼀个进程运⾏；
- 运⾏状态 -> 阻塞状态：当进程请求某个事件且必须等待时，例如请求 I/O 事件；
- 阻塞状态 -> 就绪状态：当进程要等待的事件完成时，它从阻塞状态变到就绪状态；

***

##### 进程的状态变化，会引起调度器做什么样的调度？

- 从就绪态 -> 运⾏态：当进程被创建时，会进⼊到就绪队列，从就绪态到运行态就是操作系统会从就绪队列选择⼀个进程运⾏；
- 从运⾏态 -> 阻塞态：当进程发⽣ I/O 事件⽽阻塞时，操作系统必须另外⼀个进程运⾏；
- 从运⾏态 -> 结束态：当进程退出结束后，操作系统得从就绪队列选择另外⼀个进程运⾏；

​    这些状态变化的时候，操作系统需要考虑是否要让新的进程给 CPU 运⾏，或者是否让当前进程从 CPU 上退出来⽽换另⼀个进程运⾏。

***

##### 调度器有什么调度原则？需要考虑什么指标？

​	1、如果运⾏的程序，发⽣了 I/O 事件的请求，那 CPU 使⽤率必然会很低，因为此时进程在阻塞等待硬盘的数据返回。这样的过程，势必会造成 CPU 突然的空闲。所以，为了提⾼ CPU 利⽤率，在这种发送I/O 事件致使 CPU 空闲的情况下，调度程序需要从就绪队列中选择⼀个进程来运⾏。

​	2、有的程序执⾏某个任务花费的时间会⽐较⻓，如果这个程序⼀直占⽤着CPU，会造成系统吞吐量（CPU在单位时间内完成的进程数量）的降低。所以，要提⾼系统的吞吐率，调度程序要权衡⻓任务和短任务进程的运⾏完成数量。

​	3、从进程开始到结束的过程中，实际上是包含两个时间，分别是进程运⾏时间和进程等待时间，这两个时间总和就称为周转时间。进程的周转时间越⼩越好，如果进程的等待时间很⻓⽽运⾏时间很短，那周转时间就很⻓，这不是我们所期望的，调度程序应该避免这种情况发⽣。

​	4、处于就绪队列的进程，也不能等太久，当然希望这个等待的时间越短越好，这样可以使得进程更快的在 CPU 中执⾏。所以，就绪队列中进程的等待时间也是调度程序所需要考虑的原则。

​	5、对于⿏标、键盘这种交互式⽐较强的应⽤，我们当然希望它的响应时间越快越好，否则就会影响⽤户体验了。所以，对于交互式⽐较强的应⽤，响应时间也是调度程序需要考虑的原则。

​	针对上⾯的五种调度原则，从总结成如下：

- CPU 利⽤率：调度程序应确保 CPU 是始终匆忙的状态，这可提⾼ CPU 的利⽤率；
- 系统吞吐量：吞吐量表示的是单位时间内 CPU 完成进程的数量，⻓作业的进程会占⽤较⻓的 CPU 资源，因此会降低吞吐量，相反，短作业的进程会提升系统吞吐量；
- 周转时间：周转时间是进程运⾏和阻塞时间总和，⼀个进程的周转时间越⼩越好；
- 等待时间：这个等待时间不是阻塞状态的时间，⽽是进程处于就绪队列的时间，等待的时间越⻓，⽤户越不满意；
- 响应时间：⽤户提交请求到系统第⼀次产⽣响应所花费的时间，在交互式系统中，响应时间是衡量调度算法好坏的主要标准。

***

##### 进程的调度算法有哪些？

- 先来先服务。使用此算法，将按照请求顺序为进程分配 CPU。最基本的，会有一个就绪队列。当第一个任务从外部进入系统时，将会立即启动并允许运行任意长的时间。它不会因为运行时间太长而中断。当其他作业进入时，它们排到就绪队列尾部。
- 短作业优先调度算法。是指对短作业优先调度的算法，从后备队列中选择一个或若干个估计运行时间最短的作业，将它们调入内存运行。 短作业优先调度算法是一个非抢占策略，他的原则是下一次选择预计处理时间最短的进程，因此短进程将会越过长作业，跳至队列头。
- 最短剩余时间优先算法。使用这个算法，调度程序总是选择剩余运行时间最短的那个进程运行。当一个新作业到达时，其整个时间同当前进程的剩余时间做比较。如果新的进程比当前运行进程需要更少的时间，当前进程就被挂起，而运行新的进程。这种方式能够使短期作业获得良好的服务。
- 轮询算法。每个进程都会被分配一个时间片，在这个时间片内允许进程运行。如果时间片结束时进程还在运行的话，则抢占一个 CPU 并将其分配给另一个进程。如果进程在时间片结束前阻塞或结束，则 CPU 立即进行切换。
- 优先级调度。为每个进程分配一个优先级，按优先级进行调度。为了防止低优先级的进程永远等不到调度，可以随着时间的推移增加等待进程的优先级。
- 多级反馈队列调度。是「时间⽚轮转算法」和「最⾼优先级算法」的结合。其中，「多级」表示有多个队列，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越短。「反馈」表示如果有新的进程加⼊优先级⾼的队列时，⽴刻停⽌当前正在运⾏的进程，转⽽去运⾏优先级⾼的队列；

****

##### 多级反馈队列调度算法是怎么工作的？

- 设置了多个队列，赋予每个队列不同的优先级，队列优先级从⾼到低，同时优先级越⾼时间⽚越短；

- 新的进程会被放⼊到第⼀级队列的末尾，按先来先服务的原则排队等待被调度，如果在第⼀级队列规定的时间⽚没运⾏完成，则将其转⼊到第⼆级队列的末尾，以此类推，直⾄完成；
- 当较⾼优先级的队列为空，才调度较低优先级的队列中的进程运⾏。如果进程运⾏时，有新进程进⼊较⾼优先级的队列，则停⽌当前运⾏的进程并将其移⼊到原队列末尾(因为它是低优先级队列上来的，所以要让位)，接着让较⾼优先级的进程运⾏；

​    可以发现，对于短作业可能可以在第⼀级队列很快被处理完。对于⻓作业，如果在第⼀级队列处理不完，可以移⼊下次队列等待被执⾏，虽然等待的时间变⻓了，但是运⾏时间也变更⻓了

***



#### 进程间通信

***

##### 进程间的通信方式有哪些？

​	常见的IPC有管道、命名管道、消息队列、共享内存、信号量、信号、Socket

***

##### 管道是什么？

​	管道的是进程间通信一种方式，管道的本质其实就是内核中的一块内存 (或者叫内核缓冲区)，这块缓冲区中的数据存储在一个环形队列中，因为管道在内核里边，因此我们不能直接对其进行任何操作。

***

##### 管道有什么特点？

- 管道对应的内核缓冲区大小是固定的，默认为 4k（也就是队列最大能存储 4k 数据）
- 管道分为两部分：读端和写端（队列的两端），数据从写端进入管道，从读端流出管道。
- 管道中的数据只能读一次，做一次读操作之后数据也就没有了（读数据相当于出队列）。
- 管道是单工的：数据只能单向流动，数据从写端流向读端。
- 对管道的操作（读、写）默认是阻塞的。对于读管道来说，如果管道中没有数据，读操作被阻塞，当管道中有数据之后阻塞才能解除；对于写管道来说，如果管道被写满了，写数据的操作被阻塞，当管道变为不满的状态，写阻塞解除

***

##### 匿名管道和有名管道是什么？

- 匿名管道：是管道的一种，既然是匿名也就是说这个管道没有名字，但其本质是不变的，就是位于内核中的一块内存。匿名管道只能实现有血缘关系(父子，兄弟)的进程间通信
- 命名管道：它可以在不相关的进程间也能相互通信。因为命令管道，提前创建了⼀个类型为管道的设备⽂件，在进程⾥只要使⽤这个设备⽂件，就可以相互通信。

***

##### 消息队列是什么？

​	消息队列是保存在内核中的消息链表，在发送数据时，会分成⼀个⼀个独⽴的数据单元，也就是消息体（数据块），消息体是⽤户⾃定义的数据类型，消息的发送⽅和接收⽅要约定好消息体的数据类型，所以每个消息体都是固定⼤⼩的存储块，不像管道是⽆格式的字节流数据。如果进程从消息队列中读取了消息体，内核就会把这个消息体删除。

---

##### 消息队列有什么局限性？

- **消息队列不适合⽐较⼤数据的传输**，因为在内核中每个消息体都有⼀个最⼤⻓度的限制，同时所有队列所包含的全部消息体的总⻓度也是有上限。
- **消息队列通信过程中，存在⽤户态与内核态之间的数据拷⻉开销**，因为进程写⼊数据到内核中的消息队列时，会发⽣从⽤户态拷⻉数据到内核态的过程，同理另⼀进程读取内核中的消息数据时，会发⽣从内核态拷⻉数据到⽤户态的过程。

***

##### 共享内存的实现机制是怎么样的？

​	共享内存的机制，就是拿出⼀块虚拟地址空间来，映射到相同的物理内存中。这样这个进程写⼊的东⻄，另外⼀个进程⻢上就能看到了，不需要像消息队列一样有发⽣⽤户态与内核态之间的消息拷⻉过程

***

##### 信号是什么？

​	**信号是进程间通信机制中唯⼀的异步通信机制**，因为可以在任何时候发送信号给某⼀进程，⼀旦有信号产⽣，我们就有下⾯这⼏种，⽤户进程对信号的处理⽅式。

​	1、执⾏默认操作。Linux 对每种信号都规定了默认操作，例如，SIGTERM 信号的默认操作就是终⽌进程。

​	2、捕捉信号。我们可以为信号定义⼀个信号处理函数。当信号发⽣时，我们就执⾏相应的信号处理函数。

​	3、忽略信号。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应⽤进程⽆法捕捉和忽略的，即 SIGKILL 和 SEGSTOP ，它们⽤于在任何时候中断或结束某⼀进程。

***



#### 线程同步

***

##### 互斥的概念

​	由于多线程执⾏操作共享变量的代码可能会导致竞争状态，因此我们将此段代码称为临界区，它是访问共享资源的代码⽚段，⼀定不能给多线程同时执⾏。	

​	我们希望这段代码是互斥的，也就说保证⼀个线程在临界区执⾏时，其他线程应该被阻⽌进⼊临界区，说⽩了，就是这段代码执⾏过程中，最多只能出现⼀个线程。

<img src="[OS]临界区.png" alt="avg" style="zoom:70%;">

***

##### 什么是线程同步

​	所谓同步，就是并发进程/线程在⼀些关键点上可能需要互相等待与互通消息，这种相互制约的等待与互通信息称为进程/线程同步。

***

##### 实现线程同步的主要方法

​	为了实现进程/线程间正确的协作，操作系统必须提供实现进程协作的措施和⽅法，主要的⽅法有两种：

- 锁：加锁、解锁操作；根据锁的实现，可以分为忙等待锁和无忙等待锁
- 信号量：P、V 操作；

​    这两个都可以⽅便地实现进程/线程互斥，⽽信号量⽐锁的功能更强⼀些，它还可以⽅便地实现进程/线程同步。

***

##### 忙等待锁的实现

​	在说明「忙等待锁」的实现之前，先介绍现代 CPU 体系结构提供的特殊原⼦操作指令 —— 测试和置位（Test-and-Set）指令。如果⽤ C 代码表示 Test-and-Set 指令

```php
function TestAndSet(&$old_ptr, $new){
    $old = $old_ptr;
    // 把 old_ptr 更新为 new 的值
    $old_ptr = $new;
    // 返回 old_ptr 的旧值
    return $old;
}
```

​	我们可以运⽤ Test-and-Set 指令来实现「忙等待锁」，代码如下：

```php
<?php
// 锁的标志。0表示没有线程加锁，1表示有线程加锁
$flag = 0;

/**
 * 加锁
 */
function lock(&$flag){
    while (TestAndSet($flag, 1)){

    }
    // 跳出循环，获取锁
}

/**
 * 解锁
 */
function unlock(&$flag){
    $flag = 0;
}
```

​	这段代码的执行场景有：

- 第⼀个场景是，⾸先假设⼀个线程在运⾏，调⽤ lock() ，没有其他线程持有锁，所以 flag 是 0。当调⽤ TestAndSet(flag, 1) ⽅法，返回 0，线程会跳出while循环，获取锁。同时也会原⼦的设置flag为1，标志锁已经被持有。当线程离开临界区，调⽤ unlock() 将 flag 清理为 0。
- 第⼆种场景是，当某⼀个线程已经持有锁（即 flag 为1）。本线程调⽤ lock() ，然后调⽤TestAndSet(flag, 1) ，这⼀次返回 1。只要另⼀个线程⼀直持有锁， TestAndSet() 会重复返回 1，本线程会⼀直忙等。当 flag 终于被改为 0，本线程会调⽤ TestAndSet() ，返回 0 并且原⼦地设置为 1，从⽽获得锁，进⼊临界区。

​    很明显，当获取不到锁时，线程就会⼀直 wile 循环，不做任何事情，所以就被称为「忙等待锁」，也被称为⾃旋锁

***

##### 信号量的实现机制？

​	⽤了共享内存通信⽅式，带来新的问题，那就是如果多个进程同时修改同⼀个共享内存，很有可能就冲突了。例如两个进程都同时写⼀个地址，那先写的那个进程会发现内容被别⼈覆盖了。

​	为了防⽌多进程竞争共享资源，⽽造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被⼀个进程访问。正好，信号量就实现了这⼀保护机制。

​	**信号量其实是⼀个整型的计数器，主要⽤于实现进程间的互斥与同步，⽽不是⽤于缓存进程间通信的数据。**

​	信号量表示资源的数量，控制信号量的⽅式有两种原⼦操作：

- ⼀个是 P 操作，这个操作会把信号量减去 1，相减后如果信号量 < 0，则表明资源已被占⽤，进程需阻塞等待；相减后如果信号量 >= 0，则表明还有资源可使⽤，进程可正常继续执⾏。
- 另⼀个是 V 操作，这个操作会把信号量加上 1，相加后如果信号量 <= 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运⾏；相加后如果信号量 > 0，则表明当前没有阻塞中的进程；

​    P 操作是⽤在进⼊共享资源之前，V 操作是⽤在离开共享资源之后，这两个操作是必须成对出现的。

​	接下来，举个例⼦，如果要使得两个进程互斥访问共享内存，我们可以初始化信号量为 1 。具体的过程如下：

- 进程 A 在访问共享内存前，先执⾏了 P 操作，由于信号量的初始值为 1，故在进程 A 执⾏ P 操作后信号量变为 0，表示共享资源可⽤，于是进程 A 就可以访问共享内存。
- 若此时，进程 B 也想访问共享内存，执⾏了 P 操作，结果信号量变为了 -1，这就意味着临界资源已被占⽤，因此进程 B 被阻塞。
- 直到进程 A 访问完共享内存，才会执⾏ V 操作，使得信号量恢复为 0，接着就会唤醒阻塞中的线程B，使得进程 B 可以访问共享内存，最后完成共享内存的访问后，执⾏ V 操作，使信号量恢复到初始值 1。

​    可以发现，信号初始化为 1 ，就代表着是互斥信号量，它可以保证共享内存在任何时刻只有⼀个进程在访问，这就很好的保护了共享内存。

***

##### 生产者-消费者问题

⽣产者-消费者问题描述：

- ⽣产者在⽣成数据后，放在⼀个缓冲区中；
- 消费者从缓冲区取出数据处理；
- 任何时刻，只能有⼀个⽣产者或消费者可以访问缓冲区；

我们对问题分析可以得出：

- 任何时刻只能有⼀个线程操作缓冲区，说明操作缓冲区是临界代码，需要互斥；
- 缓冲区空时，消费者必须等待⽣产者⽣成数据；缓冲区满时，⽣产者必须等待消费者取出数据。说明⽣产者和消费者需要同步。

那么我们需要三个信号量，分别是：

- 互斥信号量 mutex ：⽤于互斥访问缓冲区，初始化值为 1；
- 资源信号量 fullBuffers ：⽤于消费者询问缓冲区是否有数据，有数据则读取数据，初始化值为 0（表明缓冲区⼀开始为空）；
- 资源信号量 emptyBuffers ：⽤于⽣产者询问缓冲区是否有空位，有空位则⽣成数据，初始化值为 n（缓冲区⼤⼩）；

具体实现代码如下

<img src="[OS]生产者-消费者问题.png" alt="avg" style="zoom:70%;">

​	如果消费者线程⼀开始执⾏ P(fullBuffers) ，由于信号量 fullBuffers 初始值为 0，则此时 fullBuffers 的值从 0 变为 -1，说明缓冲区⾥没有数据，消费者只能等待。

​	接着，轮到⽣产者执⾏ P(emptyBuffers) ，表示减少 1 个空槽，如果当前没有其他⽣产者线程在临界区执⾏代码，那么该⽣产者线程就可以把数据放到缓冲区，放完后，执⾏ V(fullBuffers) ，信号量 fullBuffers从 -1 变成 0，表明有「消费者」线程正在阻塞等待数据，于是阻塞等待的消费者线程会被唤醒。

​	消费者线程被唤醒后，如果此时没有其他消费者线程在读数据，那么就可以直接进⼊临界区，从缓冲区读取数据。最后，离开临界区后，把空槽的个数 + 1。

***



#### 死锁和锁

****

##### 什么是死锁

​	当两个线程为了保护两个不同的共享资源⽽使⽤了两个互斥锁，那么这两个互斥锁应⽤不当的时候，可能会造成两个线程都在等待对⽅释放锁，在没有外⼒的作⽤下，这些线程会⼀直相互等待，就没办法继续运⾏，这种情况就是发⽣了死锁。

****

##### 产生死锁的必要条件是什么？

- 互斥条件
- 持有并等待条件
- 不可剥夺条件
- 环路等待条件；

****

##### 互斥条件是指什么？

​	互斥条件是指多个线程不能同时使⽤同⼀个资源。

​	⽐如下图，如果线程 A 已经持有的资源，不能再同时被线程 B 持有，如果线程 B 请求获取线程 A 已经占⽤的资源，那线程 B 只能等待，直到线程 A 释放了资源。

<img src="[OS]死锁必要条件-互斥条件.png" alt="avg" >

***

##### 什么是持有并等待条件？

​	持有并等待条件是指，当线程 A 已经持有了资源 1，⼜想申请资源 2，⽽资源 2 已经被线程 C 持有了，所以线程 A 就会处于等待状态，**但是线程 A 在等待资源 2 的同时并不会释放⾃⼰已经持有的资源 1**。

<img src="[OS]死锁必要条件-持有并等待条件.png" alt="avg" >

***

##### 什么是不可剥夺条件？

​	不可剥夺条件是指，当线程已经持有了资源 ，在⾃⼰使⽤完之前不能被其他线程获取，线程 B 如果也想使⽤此资源，则只能在线程 A 使⽤完并释放后才能获取

<img src="[OS]死锁必要条件-不可剥夺条件.png" alt="avg" >

***

##### 什么是环路等待条件？

​	环路等待条件指都是，在死锁发⽣的时候，两个线程获取资源的顺序构成了环形链。

​	⽐如，线程 A 已经持有资源 2，⽽想请求资源 1， 线程 B 已经获取了资源 1，⽽想请求资源 2，这就形成资源请求等待的环形图。

<img src="[OS]死锁必要条件-环路等待条件.png" alt="avg" >

***

##### 如何避免死锁问题？

​	避免死锁问题就只需要破环四个必要条件的其中一个即可，最常⻅的并且可⾏的就是使⽤资源有序分配法，来破环环路等待条件。

​	那什么是资源有序分配法呢？

​	线程 A 和 线程 B 获取资源的顺序要⼀样，当线程 A 是先尝试获取资源 A，然后尝试获取资源 B 的时候，线程 B 同样也是先尝试获取资源 A，然后尝试获取资源 B。也就是说，线程 A 和 线程 B 总是以相同的顺序申请⾃⼰想要的资源。

***

##### 互斥锁和自旋锁的实现

​	互斥锁和自旋锁是最底层的两种锁，其他高级的锁都是基于他们实现的。互斥锁和自旋锁对于加锁失败后的处理方式是不一样的：

- 互斥锁加锁失败后，线程会释放 CPU ，给其他线程；
- ⾃旋锁加锁失败后，线程会忙等待，直到它拿到锁；

1、互斥锁

​    互斥锁是⼀种「独占锁」，⽐如当线程 A 加锁成功后，此时互斥锁已经被线程 A 独占了，只要线程 A 没有释放⼿中的锁，线程 B 加锁就会失败，于是就会释放 CPU 让给其他线程，既然线程 B 释放掉了 CPU，⾃然线程 B 加锁的代码就会被阻塞。

​	对于互斥锁加锁失败⽽阻塞的现象，是由操作系统内核实现的。当加锁失败时，内核会将线程置为「睡眠」状态，等到锁被释放后，内核会在合适的时机唤醒线程，当这个线程成功获取到锁后，于是就可以继续执⾏。如下图：

<img src="[OS]互斥锁的加锁过程.png" alt="avg" style="zoom:50%;">

​	所以，互斥锁加锁失败时，会从⽤户态陷⼊到内核态，让内核帮我们切换线程，虽然简化了使⽤锁的难度，但是存在⼀定的性能开销成本。

​	那这个开销成本是什么呢？会有两次线程上下⽂切换的成本：

- 当线程加锁失败时，内核会把线程的状态从「运⾏」状态设置为「睡眠」状态，然后把 CPU 切换给其他线程运⾏；
- 接着，当锁被释放时，之前「睡眠」状态的线程会变为「就绪」状态，然后内核会在合适的时间，把CPU 切换给该线程运⾏。

2、自旋锁

​	⾃旋锁是通过 CPU 提供的 CAS 函数（Compare And Swap），在「⽤户态」完成加锁和解锁操作，不会主动产⽣线程上下⽂切换，所以相⽐互斥锁来说，会快⼀些，开销也⼩⼀些。

​	⼀般加锁的过程，包含两个步骤：

- 第⼀步，查看锁的状态，如果锁是空闲的，则执⾏第⼆步；
- 第⼆步，将锁设置为当前线程持有；

​    CAS 函数就把这两个步骤合并成⼀条硬件级指令，形成原⼦指令，这样就保证了这两个步骤是不可分割的，要么⼀次性执⾏完两个步骤，要么两个步骤都不执⾏。

​	使⽤⾃旋锁的时候，当发⽣多线程竞争锁的情况，加锁失败的线程会「忙等待」，直到它拿到锁。这⾥的「忙等待」可以⽤ while 循环等待实现，不过最好是使⽤ CPU 提供的 PAUSE 指令来实现「忙等待」，因为可以减少循环等待时的耗电量。

​	⾃旋锁是最⽐较简单的⼀种锁，⼀直⾃旋，利⽤ CPU 周期，直到锁可⽤。需要注意，在单核 CPU 上，需要抢占式的调度器（即不断通过时钟中断⼀个线程，运⾏其他线程）。否则，⾃旋锁在单 CPU 上⽆法使
⽤，因为⼀个⾃旋的线程永远不会放弃 CPU。

​	⾃旋锁开销少，在多核系统下⼀般不会主动产⽣线程切换，适合异步、协程等在⽤户态切换请求的编程⽅式，但如果被锁住的代码执⾏时间过⻓，⾃旋的线程会⻓时间占⽤ CPU 资源，所以⾃旋的时间和被锁住的代码执⾏的时间是成「正⽐」的关系，我们需要清楚的知道这⼀点。

​	⾃旋锁与互斥锁使⽤层⾯⽐较相似，但实现层⾯上完全不同：**当加锁失败时，互斥锁⽤「线程切换」来应对，⾃旋锁则⽤「忙等待」来应对。**

​	它俩是锁的最基本处理⽅式，更⾼级的锁都会选择其中⼀个来实现，⽐如读写锁既可以选择互斥锁实现，也可以基于⾃旋锁实现。

***



#### 文件系统

***

##### 什么是文件系统？

​	⽂件系统是操作系统中负责管理持久数据的⼦系统，说简单点，就是负责把⽤户的⽂件存到磁盘硬件中。⽂件系统的基本数据单位是⽂件，它的⽬的是对磁盘上的⽂件进⾏组织管理，那组织的⽅式不同，就会形成不同的⽂件系统。

***

##### 文件的基本数据结构

​	Linux ⽂件系统会为每个⽂件分配两个数据结构：索引节点（index node）和⽬录项（directoryentry），它们主要⽤来记录⽂件的元信息和⽬录层次结构。

- **索引节点**，也就是 inode，⽤来记录⽂件的元信息，⽐如 inode 编号、⽂件⼤⼩、访问权限、创建时间、修改时间、**数据在磁盘的位置**等等。索引节点是⽂件的唯⼀标识，它们之间⼀⼀对应，也同样都会被存储在硬盘中，所以索引节点同样占⽤磁盘空间。
- **⽬录项**，也就是 dentry，⽤来记录⽂件的名字、索引节点指针以及与其他⽬录项的层级关联关系。多个⽬录项关联起来，就会形成⽬录结构，但它与索引节点不同的是，**⽬录项是由内核维护的⼀个数据结构，不存放于磁盘，⽽是缓存在内存。**

​    由于索引节点唯⼀标识⼀个⽂件，⽽⽬录项记录着⽂件的名，所以⽬录项和索引节点的关系是多对⼀，也就是说，⼀个⽂件可以有多个别字。⽐如，硬链接的实现就是多个⽬录项中的索引节点指向同⼀个⽂件。

​	注意，⽬录也是⽂件，也是⽤索引节点唯⼀标识，和普通⽂件不同的是，普通⽂件在磁盘⾥⾯保存的是⽂件数据，⽽⽬录⽂件在磁盘⾥⾯保存⼦⽬录或⽂件。

****

##### ⽬录项和⽬录是⼀个东⻄吗？

​	虽然名字很相近，但是它们不是⼀个东⻄，⽬录是个⽂件，持久化存储在磁盘，⽽⽬录项是内核⼀个数据结构，缓存在内存。

​	如果查询⽬录频繁从磁盘读，效率会很低，所以内核会把已经读过的⽬录⽤⽬录项这个数据结构缓存在内存，下次再次读到相同的⽬录时，只需从内存读就可以，⼤⼤提⾼了⽂件系统的效率。

​	注意，⽬录项这个数据结构不只是表示⽬录，也是可以表示⽂件的。

***

##### 那⽂件数据是如何存储在磁盘的呢？

​	磁盘读写的最⼩单位是扇区，扇区的⼤⼩只有 512B ⼤⼩，很明显，如果每次读写都以这么⼩为单位，那这读写的效率会⾮常低。

​	所以，⽂件系统把多个扇区组成了⼀个逻辑块，每次读写的最⼩单位就是逻辑块（数据块），Linux 中的逻辑块⼤⼩为 4KB ，也就是⼀次性读写 8 个扇区，这将⼤⼤提⾼了磁盘的读写的效率。

​	以上就是索引节点、⽬录项以及⽂件数据的关系，下⾯这个图就很好的展示了它们之间的关系：

<img src="[OS]索引节点-目录项-文件数据的关系.png" alt="avg" style="zoom:50%;">

​	索引节点是存储在硬盘上的数据，那么为了加速⽂件的访问，通常会把索引节点加载到内存中。

​	另外，磁盘进⾏格式化的时候，会被分成三个存储区域，**分别是超级块、索引节点区和数据块区。**

- 超级块，⽤来存储⽂件系统的详细信息，⽐如块个数、块⼤⼩、空闲块等等。
- 索引节点区(inode)，⽤来存储索引节点；
- 数据块区，⽤来存储⽂件或⽬录数据；

​    我们不可能把超级块和索引节点区全部加载到内存，这样内存肯定撑不住，所以只有当需要使⽤的时候，才将其加载进内存，它们加载进内存的时机是不同的：

- 超级块：当⽂件系统挂载时进⼊内存；
- 索引节点区：当⽂件被访问时进⼊内存；

****

##### 什么是虚拟文件系统？

​	⽂件系统的种类众多，⽽操作系统希望对⽤户提供⼀个统⼀的接⼝，于是在⽤户层与⽂件系统层引⼊了中间层，这个中间层就称为虚拟⽂件系统（Virtual File System，VFS）。

​	VFS 定义了⼀组所有⽂件系统都⽀持的数据结构和标准接⼝，这样程序员不需要了解⽂件系统的⼯作原理，只需要了解 VFS 提供的统⼀接⼝即可。

​	在 Linux ⽂件系统中，⽤户空间、系统调⽤、虚拟机⽂件系统、缓存、⽂件系统以及存储之间的关系如下图：

<img src="[OS]文件系统关系图.png" alt="avg" style="zoom:50%;">

***

##### 什么是文件打开表？

​	我们打开了⼀个⽂件后，操作系统会跟踪进程打开的所有⽂件，所谓的跟踪呢，就是操作系统为每个进程维护⼀个打开⽂件表，⽂件表⾥的每⼀项代表「⽂件描述符」，所以说⽂件描述符是打开⽂件的标识。

​	操作系统在打开⽂件表中维护着打开⽂件的状态和信息：

- ⽂件指针：系统跟踪上次读写位置作为当前⽂件位置指针，这种指针对打开⽂件的某个进程来说是唯⼀的；
- ⽂件打开计数器：⽂件关闭时，操作系统必须重⽤其打开⽂件表条⽬，否则表内空间不够⽤。因为多个进程可能打开同⼀个⽂件，所以系统在删除打开⽂件条⽬之前，必须等待最后⼀个进程关闭⽂件，该计数器跟踪打开和关闭的数量，当该计数为 0 时，系统关闭⽂件，删除该条⽬；
- ⽂件磁盘位置：绝⼤多数⽂件操作都要求系统修改⽂件数据，该信息保存在内存中，以免每个操作都从磁盘中读取；
- 访问权限：每个进程打开⽂件都需要有⼀个访问模式（创建、只读、读写、添加等），该信息保存在进程的打开⽂件表中，以便操作系统能允许或拒绝之后的 I/O 请求；

***

##### 用户和操作系统对文件的读写操作是有差异的，文件系统是如何屏蔽这种差异？

​	在⽤户视⻆⾥，⽂件就是⼀个持久化的数据结构，但操作系统并不会关⼼你想存在磁盘上的任何的数据结构，操作系统的视⻆是如何把⽂件数据和磁盘块对应起来。

​	所以，⽤户和操作系统对⽂件的读写操作是有差异的，⽤户习惯以字节的⽅式读写⽂件，⽽操作系统则是以数据块来读写⽂件，那屏蔽掉这种差异的⼯作就是⽂件系统了。

​	我们来分别看⼀下，读⽂件和写⽂件的过程：

- 当⽤户进程从⽂件读取 1 个字节⼤⼩的数据时，⽂件系统则需要获取字节所在的数据块，再返回数据块对应的⽤户进程所需的数据部分。
- 当⽤户进程把 1 个字节⼤⼩的数据写进⽂件时，⽂件系统则找到需要写⼊数据的数据块的位置，然后修改数据块中对应的部分，最后再把数据块写回磁盘。

​    所以说，**⽂件系统的基本操作单位是数据块。**

***

##### 文件都有什么存储方式？

​	⽂件的数据是要存储在硬盘上⾯的，数据在磁盘上的存放⽅式，就像程序在内存中存放的⽅式那样，有以下两种：

- 连续空间存放⽅式
- ⾮连续空间存放⽅式

​    其中，⾮连续空间存放⽅式⼜可以分为「链表⽅式」和「索引⽅式」。不同的存储⽅式，有各⾃的特点，重点是要分析它们的存储效率和读写性能。

***

##### 连续空间存放方式是怎么样的？

​	连续空间存放⽅式顾名思义，**⽂件存放在磁盘「连续的」物理空间中。**这种模式下，⽂件的数据都是紧密相连，读写效率很⾼，因为⼀次磁盘寻道就可以读出整个⽂件。

​	使⽤连续存放的⽅式有⼀个前提，必须先知道⼀个⽂件的⼤⼩，这样⽂件系统才会根据⽂件的⼤⼩在磁盘上找到⼀块连续的空间分配给⽂件。

​	所以，**⽂件头(类似inode)⾥需要指定「起始块的位置」和「⻓度」**，有了这两个信息就可以很好的表示⽂件存放⽅式是⼀块连续的磁盘空间。

​	连续空间存放的⽅式虽然读写效率⾼，**但是有「磁盘空间碎⽚」和「⽂件⻓度不易扩展」的缺陷**。

​	如下图，如果⽂件 B 被删除，磁盘上就留下⼀块空缺，这时，如果新来的⽂件⼩于其中的⼀个空缺，我们就可以将其放在相应空缺⾥。但如果该⽂件的⼤⼩⼤于所有的空缺，但却⼩于空缺⼤⼩之和，则虽然磁盘上有⾜够的空缺，但该⽂件还是不能存放。当然了，我们可以通过将现有⽂件进⾏挪动来腾出空间以容纳新的⽂件，但是这个在磁盘挪动⽂件是⾮常耗时，所以这种⽅式不太现实。

<img src="[OS]文件系统-连续空间存放方式.png" alt="avg" style="zoom:50%;">

​	另外⼀个缺陷是⽂件⻓度扩展不⽅便，例如上图中的⽂件 A 要想扩⼤⼀下，需要更多的磁盘空间，唯⼀的办法就只能是挪动的⽅式，前⾯也说了，这种⽅式效率是⾮常低的。

***

##### 非连续空间中的链表方式，是如何实现的？

​	链表的⽅式存放是**离散的，不⽤连续的**，于是就可以消除磁盘碎⽚，可⼤⼤提⾼磁盘空间的利⽤率，同时⽂件的⻓度可以动态扩展。根据实现的⽅式的不同，链表可分为**「隐式链表」**和**「显式链接」**两种形式。

​	⽂件要以**「隐式链表」**的⽅式存放的话，**实现的⽅式是⽂件头要包含「第⼀块」和「最后⼀块」的位置，并且每个数据块⾥⾯留出⼀个指针空间，⽤来存放下⼀个数据块的位置**，这样⼀个数据块连着⼀个数块，从链头开是就可以顺着指针找到所有的数据块，所以存放的⽅式可以是不连续的。<img src="[OS]文件系统-非连续空间存放方式-隐式链表.png" alt="[OS]文件系统-非连续空间存放方式-隐式链表" style="zoom:70%;" />

​	隐式链表的存放⽅式的**缺点在于⽆法直接访问数据块，只能通过指针顺序访问⽂件，以及数据块指针消耗了⼀定的存储空间**。隐式链接分配的稳定性较差，系统在运⾏过程中由于软件或者硬件错误导致链表中的指针丢失或损坏，会导致⽂件数据的丢失。

​	如果取出每个磁盘块的指针，把它放在内存的⼀个表中，就可以解决上述隐式链表的两个不⾜。那么，这种实现⽅式是**「显式链接」**，它指**把⽤于链接⽂件各数据块的指针，显式地存放在内存的⼀张链接表中**，该表在整个磁盘仅设置⼀张，**每个表项中存放链接指针，指向下⼀个数据块号。**

​	对于显式链接的⼯作⽅式，我们举个例⼦，⽂件 A 依次使⽤了磁盘块 4、7、2、10 和 12 ，⽂件 B 依次使⽤了磁盘块 6、3、11 和 14 。利⽤下图中的表，可以从第 4 块开始，顺着链⾛到最后，找到⽂件 A 的全部磁盘块。同样，从第 6 块开始，顺着链⾛到最后，也能够找出⽂件 B 的全部磁盘块。最后，这两个链都以⼀个不属于有效磁盘编号的特殊标记（如 -1 ）结束。内存中的这样⼀个表格称为⽂件分配表（File Allocation Table，FAT）。

<img src="[OS]文件系统-非连续空间存放方式-显示链表-文件分配表.png" alt="[OS]文件系统-非连续空间存放方式-显示链表-文件分配表" style="zoom:50%;" />

​	由于查找记录的过程是在内存中进⾏的，因⽽不仅显著地**提⾼了检索速度**，⽽且**⼤⼤减少了访问磁盘的次数。**但也正是整个表都存放在内存中的关系，它的主要的缺点是**不适⽤于⼤磁盘。**

​	⽐如，对于 200GB 的磁盘和 1KB ⼤⼩的块，这张表需要有 2 亿项，每⼀项对应于这 2 亿个磁盘块中的⼀个块，每项如果需要 4 个字节，那这张表要占⽤ 800MB 内存，很显然 FAT ⽅案对于⼤磁盘⽽⾔不太合适。

***

##### 非连续空间中的索引方式，是如何实现的？

​	链表的⽅式解决了连续分配的磁盘碎⽚和⽂件动态扩展的问题，但是不能有效⽀持直接访问（FAT除外），索引的⽅式可以解决这个问题。

​	索引的实现是为每个⽂件创建⼀个**「索引数据块」**，⾥⾯存放的是**指向⽂件数据块的指针列表**，说⽩了就像书的⽬录⼀样，要找哪个章节的内容，看⽬录查就可以。

​	另外，**⽂件头需要包含指向「索引数据块」的指针**，这样就可以通过⽂件头知道索引数据块的位置，再通过索引数据块⾥的索引信息找到对应的数据块。

​	创建⽂件时，索引块的所有指针都设为空。当⾸次写⼊第 i 块时，先从空闲空间中取得⼀个块，再将其地址写到索引块的第 i 个条⽬。

<img src="[OS]文件系统-非连续空间存放方式-索引方式.png" alt="[OS]文件系统-非连续空间存放方式-索引方式" style="zoom:80%;" />

​	索引的⽅式优点在于：

- ⽂件的创建、增⼤、缩⼩很⽅便；
- 不会有碎⽚的问题；
- 持顺序读写和随机读写；

​    由于索引数据也是存放在磁盘块的，如果⽂件很⼩，明明只需⼀块就可以存放的下，但还是需要额外分配⼀块来存放索引数据，所以缺陷之⼀就是存储索引带来的开销。

​	如果⽂件很⼤，⼤到⼀个索引数据块放不下索引信息，这时⼜要如何处理⼤⽂件的存放呢？我们可以通过组合的⽅式，来处理⼤⽂件的存放。

​	先来看看链表 + 索引的组合，这种组合称为**「链式索引块」**，它的实现⽅式是**在索引数据块留出⼀个存放下⼀个索引数据块的指针**，于是当⼀个索引数据块的索引信息⽤完了，就可以通过指针的⽅式，找到下⼀个索引数据块的信息。那这种⽅式也会出现前⾯提到的链表⽅式的问题，万⼀某个指针损坏了，后⾯的数据也就会⽆法读取了。

​	<img src="[OS]文件系统-非连续空间存放方式-索引方式-链式索引块.png" alt="[OS]文件系统-非连续空间存放方式-索引方式-链式索引块" style="zoom:80%;" />

​	还有另外⼀种组合⽅式是索引 + 索引的⽅式，这种组合称为**「多级索引块」**，实现⽅式是**通过⼀个索引块来存放多个索引数据块**，⼀层套⼀层索引

<img src="[OS]文件系统-非连续空间存放方式-索引方式-多级索引块.png" alt="[OS]文件系统-非连续空间存放方式-索引方式-多级索引块" style="zoom:80%;" />

***

##### 空闲空间管理通常由什么管理方式？

​	⽂件的存储是针对已经被占⽤的数据块组织和管理，而空闲管理就是针对磁盘的空闲空间的管理机制，常见的管理机制有：空闲表法、空闲链表法、位图法

**1、空闲表法**

​	空闲表法就是为所有空闲空间建⽴⼀张表，表内容包括空闲区的第⼀个块号和该空闲区的块个数，注意，这个⽅式是连续分配的。如下图：

<img src="[OS]文件系统-空闲空间管理-空闲表法.png" alt="[OS]文件系统-空闲空间管理-空闲表法" style="zoom:50%;" />

​	当请求分配磁盘空间时，系统依次扫描空闲表⾥的内容，直到找到⼀个合适的空闲区域为⽌。当⽤户撤销⼀个⽂件时，系统回收⽂件空间。这时，也需顺序扫描空闲表，寻找⼀个空闲表条⽬并将释放空间的第⼀个物理块号及它占⽤的块数填到这个条⽬中。

​	这种⽅法仅当有少量的空闲区时才有较好的效果。因为，如果存储空间中有着⼤量的⼩的空闲区，则空闲表变得很⼤，这样查询效率会很低。另外，这种分配技术适⽤于建⽴连续⽂件。

**2、空闲链表法**

​	我们也可以使⽤「链表」的⽅式来管理空闲空间，每⼀个空闲块⾥有⼀个指针指向下⼀个空闲块，这样也能很⽅便的找到空闲块并管理起来。如下图：

​	<img src="[OS]文件系统-空闲空间管理-空闲链表法.png" alt="[OS]文件系统-空闲空间管理-空闲链表法" style="zoom:80%;" />

​	当创建⽂件需要⼀块或⼏块时，就从链头上依次取下⼀块或⼏块。反之，当回收空间时，把这些空闲块依次接到链头上。

​	这种技术只要在主存中保存⼀个指针，令它指向第⼀个空闲块。其特点是简单，但不能随机访问，⼯作效率低，同时数据块的指针消耗了⼀定的存储空间。

​	空闲表法和空闲链表法都不适合⽤于⼤型⽂件系统，因为这会使空闲表或空闲链表太⼤。

**3、位图法**

​	位图是利⽤⼆进制的⼀位来表示磁盘中⼀个盘块的使⽤情况，磁盘上所有的盘块都有⼀个⼆进制位与之对应。

​	当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下：

​	在 Linux ⽂件系统就采⽤了位图的⽅式来管理空闲空间，不仅⽤于数据空闲块的管理，还⽤于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，⾃然也要有对其管理。

***

##### 目录的存储方式是怎么样的？

​	和普通⽂件不同的是，**普通⽂件的块⾥⾯保存的是⽂件数据，⽽⽬录⽂件的块⾥⾯保存的是⽬录⾥⾯⼀项⼀项的⽂件信息。**

​	在⽬录⽂件的块中，最简单的保存格式就是列表，就是⼀项⼀项地将⽬录下的⽂件信息（如⽂件名、⽂件inode、⽂件类型等）列在表⾥。

​	列表中每⼀项就代表该⽬录下的⽂件的⽂件名和对应的 inode，通过这个 inode，就可以找到真正的⽂件。

<img src="[OS]文件系统-目录的存储方式.png" alt="[OS]文件系统-目录的存储方式" style="zoom:50%;" />

​	通常，第⼀项是「 . 」，表示当前⽬录，第⼆项是「 .. 」，表示上⼀级⽬录，接下来就是⼀项⼀项的⽂件名和 inode。

​	如果⼀个⽬录有超级多的⽂件，我们要想在这个⽬录下找⽂件，按照列表⼀项⼀项的找，效率就不⾼了。于是，保存⽬录的格式改成哈希表，对⽂件名进⾏哈希计算，把哈希值保存起来，如果我们要查找⼀个⽬录下⾯的⽂件名，可以通过名称取哈希。如果哈希能够匹配上，就说明这个⽂件的信息在相应的块⾥⾯。

***

##### 软链接和硬链接是什么 ？

​	有时候我们希望给某个⽂件取个别名，那么在 Linux 中可以通过硬链接（Hard Link） 和软链接（Symbolic Link） 的⽅式来实现，它们都是⽐较特殊的⽂件，但是实现⽅式也是不相同的。

​	硬链接是**多个⽬录项中的「索引节点」指向⼀个⽂件**，也就是指向同⼀个 inode，但是 inode 是不可能跨越⽂件系统的，每个⽂件系统都有各⾃的 inode 数据结构和列表，所以**硬链接是不可⽤于跨⽂件系统的**。由于多个⽬录项都是指向⼀个 inode，那么**只有删除⽂件的所有硬链接以及源⽂件时，系统才会彻底删除该⽂件。**

​	<img src="[OS]文件系统-硬链接.png" alt="[OS]文件系统-硬链接" style="zoom:50%;" />

​	

​	软链接相当于重新创建⼀个⽂件(类似索引)，这个⽂件有**独⽴的 inode**，但是这个**⽂件的内容是另外⼀个⽂件的路径**，所以访问软链接的时候，实际上相当于访问到了另外⼀个⽂件，所以**软链接是可以跨⽂件系统的**，甚⾄**⽬标⽂件被删除了，链接⽂件还是在的，只不过指向的⽂件找不到了⽽已。**

<img src="[OS]文件系统-软链接.png" alt="[OS]文件系统-软链接" style="zoom:50%;" />

***

##### 缓冲IO和非缓冲IO

​	⽂件操作的标准库是可以实现数据的缓存，那么根据**「是否利⽤标准库缓冲」，可以把⽂件 I/O 分为缓冲I/O 和⾮缓冲 I/O**：

- 缓冲 I/O，利⽤的是标准库的缓存实现⽂件的加速访问，⽽标准库再通过系统调⽤访问⽂件。
- ⾮缓冲 I/O，直接通过系统调⽤访问⽂件，不经过标准库缓存。

​    这⾥所说的「缓冲」特指标准库内部实现的缓冲。⽐⽅说，很多程序遇到换⾏时才真正输出，⽽换⾏前的内容，其实就是被标准库暂时缓存了起来，这样做的⽬的是，减少系统调⽤的次数，毕竟系统调⽤是有 CPU 上下⽂切换的开销的。

***

##### 直接与⾮直接 I/O

​	我们都知道磁盘 I/O 是⾮常慢的，所以 Linux 内核为了减少磁盘 I/O 次数，在系统调⽤后，会把⽤户数据拷⻉到内核中缓存起来，这个内核缓存空间也就是「⻚缓存」，只有当缓存满⾜某些条件的时候，才发起磁盘 I/O 的请求。

​	那么，根据「是否利⽤操作系统的缓存」，可以把⽂件 I/O 分为直接 I/O 与⾮直接 I/O：

- 直接 I/O，不会发⽣内核缓存和⽤户程序之间数据复制，⽽是直接经过⽂件系统访问磁盘。
- ⾮直接 I/O，读操作时，数据从内核缓存中拷⻉给⽤户程序，写操作时，数据从⽤户程序拷⻉给内核缓存，再由内核决定什么时候写⼊数据到磁盘。

​    如果你在使⽤⽂件操作类的系统调⽤函数时，指定了 O_DIRECT 标志，则表示使⽤直接 I/O。如果没有设置过，默认使⽤的是⾮直接 I/O。

****

##### 如果⽤了⾮直接 I/O 进⾏写数据操作，内核什么情况下才会把缓存数据写⼊到磁盘？

​	以下⼏种场景会触发内核缓存的数据写⼊磁盘：

- 在调⽤ write 的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上；
- ⽤户主动调⽤ sync ，内核缓存会刷到磁盘上；
- 当内存⼗分紧张，⽆法再分配⻚⾯时，也会把内核缓存的数据刷到磁盘上；
- 内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上；

***

##### 同步、异步、阻塞、非阻塞的概念

​	根据应用程序是否阻塞自身运行，可以把文件 I/O 分为阻塞 I/O 和非阻塞 I/O：

- 所谓阻塞 I/O，是指应用程序执行 I/O 操作后，如果没有获得响应，就会阻塞当前线程，自然就不能执行其他任务。
- 所谓非阻塞 I/O，是指应用程序执行 I/O 操作后，不会阻塞当前的线程，可以继续执行其他的任务，随后再通过轮询或者事件通知的形式，获取调用的结果。

​    根据是否等待响应结果，可以把文件 I/O 分为同步和异步 I/O：

- 所谓同步 I/O，是指应用程序执行 I/O 操作后，要一直等到整个 I/O 完成后，才能获得 I/O 响应。
- 所谓异步 I/O，是指应用程序执行 I/O 操作后，不用等待完成和完成后的响应，而是继续执行就可以。等到这次 I/O 完成后，响应会用事件通知的方式，告诉应用程序。

****



#### 设备管理

***

##### 什么是设备控制器？

​	我们的电脑设备可以接⾮常多的输⼊输出设备，⽐如键盘、⿏标、显示器、⽹卡、硬盘、打印机、⾳响等等，每个设备的⽤法和功能都不同，那操作系统是如何把这些输⼊输出设备统⼀管理的呢?

​	为了屏蔽设备之间的差异，每个设备都有⼀个叫设备控制器（Device Control） 的组件，⽐如硬盘有硬盘控制器、显示器有视频控制器等。

​	因为这些控制器都很清楚的知道对应设备的⽤法和功能，所以 CPU 是通过设备控制器来和设备打交道的。

​	设备控制器⾥有芯⽚，它可执⾏⾃⼰的逻辑，也有⾃⼰的寄存器，⽤来与 CPU 进⾏通信，⽐如：

- 通过写⼊这些寄存器，操作系统可以命令设备发送数据、接收数据、开启或关闭，或者执⾏某些其他操作。
- 通过读取这些寄存器，操作系统可以了解设备的状态，是否准备好接收⼀个新的命令等。

​    实际上，控制器是有三类寄存器，它们分别是状态寄存器（Status Register）、 命令寄存器（Command Register）以及数据寄存器（Data Register）

- 数据寄存器，CPU 向 I/O 设备写⼊需要传输的数据，⽐如要打印的内容是「Hello」，CPU 就要先发送⼀个 H 字符给到对应的 I/O 设备。
- 命令寄存器，CPU 发送⼀个命令，告诉 I/O 设备，要进⾏输⼊/输出操作，于是就会交给 I/O 设备去⼯作，任务完成后，会把状态寄存器⾥⾯的状态标记为完成。
- 状态寄存器，⽬的是告诉 CPU ，现在已经在⼯作或⼯作已经完成，如果已经在⼯作状态，CPU 再发送数据或者命令过来，都是没有⽤的，直到前⾯的⼯作已经完成，状态寄存标记成已完成，CPU 才能发送下⼀个字符和命令。

​    CPU 通过读写设备控制器中的寄存器控制设备，这可⽐ CPU 直接控制输⼊输出设备，要⽅便和标准很多。

***

##### 什么是块设备和字符设备？

​	输⼊输出设备可分为两⼤类 ：块设备（Block Device）和字符设备（Character Device）。

- 块设备，把数据存储在固定⼤⼩的块中，每个块有⾃⼰的地址，硬盘、USB 是常⻅的块设备。
- 字符设备，以字符为单位发送或接收⼀个字符流，字符设备是不可寻址的，也没有任何寻道操作，⿏标是常⻅的字符设备。

​    块设备通常传输的数据量会⾮常⼤，于是控制器设⽴了⼀个可读写的数据缓冲区。

- CPU 写⼊数据到控制器的缓冲区时，当缓冲区的数据囤够了⼀部分，才会发给设备。
- CPU 从控制器的缓冲区读取数据时，也需要缓冲区囤够了⼀部分，才拷⻉到内存。

​    这样做是为了，减少对设备的频繁操作。

***

#####  CPU 是如何与设备的控制寄存器和数据缓冲区进⾏通信的？

- 端⼝ I/O，每个控制寄存器被分配⼀个 I/O 端⼝，可以通过特殊的汇编指令操作这些寄存器，⽐如in/out 类似的指令。
- 内存映射 I/O，将所有控制寄存器映射到内存空间中，这样就可以像读写内存⼀样读写数据缓冲区。

***

##### 当设备处理完任务后，如何通知CPU呢？

​	当设备完成任务后触发中断到中断控制器，中断控制器就通知 CPU，⼀个中断产⽣了，CPU 需要停下当前⼿⾥的事情来处理中断。

​	另外，中断有两种，⼀种软中断，例如代码调⽤ INT 指令触发，⼀种是硬件中断，就是硬件通过中断控制器触发的

***

##### 什么是通用块层？

​	对于块设备，为了减少不同块设备的差异带来的影响，Linux 通过⼀个统⼀的通⽤块层，来管理不同的块设备。

​	通⽤块层是处于⽂件系统和磁盘驱动中间的⼀个块设备抽象层，它主要有两个功能：

​	1、**向上为⽂件系统和应⽤程序，提供访问块设备的标准接⼝，向下把各种不同的磁盘设备抽象为统⼀的块设备**，并在内核层⾯，提供⼀个框架来管理这些设备的驱动程序；

​	2、通⽤层还会给⽂件系统和应⽤程序发来的 I/O 请求排队，接着会对队列重新排序、请求合并等⽅式，也就是 I/O 调度，主要⽬的是为了提⾼磁盘读写的效率。

***



#### 网络

***

##### 为什么需要DMA技术？

​	在没有 DMA 技术前，I/O 的过程是这样的：

- CPU 发出对应的指令给磁盘控制器，然后返回；
- 磁盘控制器收到指令后，于是就开始准备数据，会把数据放⼊到磁盘控制器的内部缓冲区中，然后产⽣⼀个中断；
- CPU 收到中断信号后，停下⼿头的⼯作，接着把磁盘控制器的缓冲区的数据⼀次⼀个字节地读进⾃⼰的寄存器，然后再把寄存器⾥的数据写⼊到内存，⽽在数据传输的期间 CPU 是⽆法执⾏其他任务的。

<img src="[OS]网络-无DMA数据传输.png" alt="[OS]网络-无DMA数据传输" style="zoom:50%;" />

​	简单的搬运⼏个字符数据那没问题，但是如果我们⽤千兆⽹卡或者硬盘传输⼤量数据的时候，都⽤ CPU 来搬运的话，肯定忙不过来。

​	计算机科学家们发现了事情的严重性后，于是就发明了 DMA 技术，也就是直接内存访问（DirectMemory Access） 技术。

​	什么是 DMA 技术？简单理解就是，在进⾏ I/O 设备和内存的数据传输的时候，数据搬运的⼯作全部交给DMA 控制器，⽽ CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务。

​	那使⽤ DMA 控制器进⾏数据传输的过程究竟是什么样的呢？下⾯我们来具体看看。

<img src="[OS]网络-有DMA数据传输.png" alt="[OS]网络-有DMA数据传输" style="zoom:50%;" />

​	具体过程如下：

- ⽤户进程调⽤ read ⽅法，向操作系统发出 I/O 请求，请求读取数据到⾃⼰的内存缓冲区中，进程进⼊阻塞状态；
- 操作系统收到请求后，进⼀步将 I/O 请求发送 DMA，然后让 CPU 执⾏其他任务；DMA 进⼀步将 I/O 请求发送给磁盘；
- **磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知⾃⼰缓冲区已满；**
- DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷⻉到内核缓冲区中，此时不占⽤ CPU，CPU可以执⾏其他任务；
- CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷⻉到⽤户空间，系统调⽤返回；

​    可以看到， 整个数据传输的过程，CPU 不再参与数据搬运的⼯作，⽽是全程由 DMA 完成，但是 CPU 在这个过程中也是必不可少的，因为传输什么数据，从哪⾥传输到哪⾥，都需要 CPU 来告诉 DMA 控制器。

​	早期 DMA 只存在在主板上，如今由于 I/O 设备越来越多，数据传输的需求也不尽相同，所以每个 I/O 设备⾥⾯都有⾃⼰的 DMA 控制器。

***

##### 传统的文件传输过程是怎么样的？

​	如果服务端要提供⽂件传输的功能，我们能想到的最简单的⽅式是：将磁盘上的⽂件读取出来，然后通过⽹络协议发送给客户端。

​	传统 I/O 的⼯作⽅式是，数据读取和写⼊是从⽤户空间到内核空间来回复制，⽽内核空间的数据是通过操作系统层⾯的 I/O 接⼝从磁盘读取或写⼊。

​	⼀般会需要 read 和 write 两个系统调⽤，代码很简单，虽然就两⾏代码，但是这⾥⾯发⽣了不少的事情

​    <img src="[OS]网络-传统文件传输过程.png" alt="[OS]网络-传统文件传输过程" style="zoom:50%;" />

​	⾸先，期间共发⽣了 4 次⽤户态与内核态的上下⽂切换，因为发⽣了两次系统调⽤，⼀次是 read() ，⼀次是 write() ，每次系统调⽤都得先从⽤户态切换到内核态，等内核完成任务后，再从内核态切换回⽤户态。

​	上下⽂切换到成本并不⼩，⼀次切换需要耗时⼏⼗纳秒到⼏微秒，虽然时间看上去很短，但是在⾼并发的场景下，这类时间容易被累积和放⼤，从⽽影响系统的性能。

​	其次，还发⽣了 4 次数据拷⻉，其中两次是 DMA 的拷⻉，另外两次则是通过 CPU 拷⻉的，下⾯说⼀下这个过程：

​	1、把磁盘上的数据拷⻉到操作系统内核的缓冲区⾥，这个拷⻉的过程是通过 DMA 搬运的。

​	2、把内核缓冲区的数据拷⻉到⽤户的缓冲区⾥，于是我们应⽤程序就可以使⽤这部分数据了，这个拷⻉到过程是由 CPU 完成的。

​	3、把刚才拷⻉到⽤户的缓冲区⾥的数据，再拷⻉到内核的 socket 的缓冲区⾥，这个过程依然还是由 CPU 搬运的。

​	4、把内核的 socket 缓冲区⾥的数据，拷⻉到⽹卡的缓冲区⾥，这个过程⼜是由 DMA 搬运的。

​	我们回过头看这个⽂件传输的过程，我们只是搬运⼀份数据，结果却搬运了 4 次，过多的数据拷⻉⽆疑会消耗 CPU 资源，⼤⼤降低了系统性能。

​	这种简单⼜传统的⽂件传输⽅式，存在冗余的上⽂切换和数据拷⻉，在⾼并发系统⾥是⾮常糟糕的，多了很多不必要的开销，会严重影响系统性能。

​	所以，要想提⾼⽂件传输的性能，**就需要减少「⽤户态与内核态的上下⽂切换」和「内存拷⻉」的次数。**

***

##### 如何减少「⽤户态与内核态的上下⽂切换」的次数呢？

​	读取磁盘数据的时候，之所以要发⽣上下⽂切换，这是因为⽤户空间没有权限操作磁盘或⽹卡，内核的权限最⾼，这些操作设备的过程都需要交由操作系统内核来完成，所以⼀般要通过内核去完成某些任务的时候，就需要使⽤操作系统提供的系统调⽤函数。

​	⽽⼀次系统调⽤必然会发⽣ 2 次上下⽂切换：⾸先从⽤户态切换到内核态，当内核执⾏完任务后，再切换回⽤户态交由进程代码执⾏。

​	所以，要想减少上下⽂切换到次数，就要减少系统调⽤的次数。

***

##### 如何减少「数据拷⻉」的次数？

​	在前⾯我们知道了，传统的⽂件传输⽅式会历经 4 次数据拷⻉，⽽且这⾥⾯，「从内核的读缓冲区拷⻉到⽤户的缓冲区⾥，再从⽤户的缓冲区⾥拷⻉到 socket 的缓冲区⾥」，这个过程是没有必要的。

​	因为⽂件传输的应⽤场景中，在⽤户空间我们并不会对数据「再加⼯」，所以数据实际上可以不⽤搬运到⽤户空间，因此⽤户的缓冲区是没有必要存在的。

***

##### 如何实现零拷贝？

​	在 Linux 内核版本 2.1 中，提供了⼀个专⻔发送⽂件的系统调⽤函数 sendfile() ，它可以替代前⾯的 read() 和 write() 这两个系统调⽤，这样就可以减少⼀次系统调⽤，也就减少了 2 次上下⽂切换的开销。而且，该系统调⽤，可以直接把内核缓冲区⾥的数据拷⻉到 socket 缓冲区⾥，不再拷⻉到⽤户态，这样就只有 2 次上下⽂切换，和 3 次数据拷⻉。如下图：

<img src="[OS]网络-sendfile拷贝过程.png" alt="[OS]网络-sendfile拷贝过程" style="zoom:50%;" />

​	但是这还不是真正的零拷⻉技术，如果⽹卡⽀持 SG-DMA（The Scatter-Gather Direct Memory Access）技术（和普通的 DMA 有所不同），我们可以进⼀步减少通过 CPU 把内核缓冲区⾥的数据拷⻉到 socket 缓冲区的过程。

​	于是，从 Linux 内核 2.4 版本开始起，对于⽀持⽹卡⽀持 SG-DMA 技术的情况下， sendfile() 系统调⽤的过程发⽣了点变化，具体过程如下：

​	1、通过 DMA 将磁盘上的数据拷⻉到内核缓冲区⾥；

​	2、缓冲区描述符和数据⻓度传到 socket 缓冲区，这样⽹卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷⻉到⽹卡的缓冲区⾥，此过程不需要将数据从操作系统内核缓冲区拷⻉到 socket缓冲区中，这样就减少了⼀次数据拷⻉；

<img src="[OS]网络-sendfile拷贝过程-优化.png" alt="[OS]网络-sendfile拷贝过程-优化" style="zoom:50%;" />

​	这就是所谓的**零拷⻉（Zero-copy）技术，因为我们没有在内存层⾯去拷⻉数据，也就是说全程没有通过CPU 来搬运数据，所有的数据都是通过 DMA 来进⾏传输的。**

​	零拷⻉技术的⽂件传输⽅式相⽐传统⽂件传输的⽅式，减少了 2 次上下⽂切换和数据拷⻉次数，**只需要 2 次上下⽂切换和数据拷⻉次数，就可以完成⽂件的传输，⽽且 2 次的数据拷⻉过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。**

​	所以，总体来看，零拷⻉技术可以把⽂件传输的性能提⾼⾄少⼀倍以上。

***

##### PageCache 有什么作⽤？

​	在⽂件传输过程，其中第⼀步都是先需要先把磁盘⽂件数据拷⻉「内核缓冲区」⾥，这个「内核缓冲区」实际上是磁盘⾼速缓存（PageCache）。

​	读写磁盘相⽐读写内存的速度慢太多了，所以我们应该想办法把「读写磁盘」替换成「读写内存」。于是，我们会通过 DMA 把磁盘⾥的数据搬运到内存⾥，这样就可以⽤读内存替换读磁盘。

​	但是，内存空间远⽐磁盘要⼩，内存注定只能拷⻉磁盘⾥的⼀⼩部分数据。那问题来了，选择哪些磁盘数据拷⻉到内存呢？

​	我们都知道程序运⾏的时候，具有「局部性」，所以通常，刚被访问的数据在短时间内再次被访问的概率很⾼，于是我们可以⽤ PageCache 来缓存最近被访问的数据，当空间不⾜时淘汰最久未被访问的缓存。所以，读磁盘数据的时候，优先在 PageCache 找，如果数据存在则可以直接返回；如果没有，则从磁盘中读取，然后缓存 PageCache 中。

​	还有⼀点，读取磁盘数据的时候，需要找到数据所在的位置，但是对于机械磁盘来说，就是通过磁头旋转到数据所在的扇区，再开始「顺序」读取数据，但是旋转磁头这个物理动作是⾮常耗时的，为了降低它的影响，PageCache 使⽤了「预读功能」。

​	⽐如，假设 read ⽅法每次只会读 32 KB 的字节，虽然 read 刚开始只会读 0 ～ 32 KB 的字节，但内核会把其后⾯的 32～64 KB 也读取到 PageCache，这样后⾯读取 32～64 KB 的成本就很低，如果在 32～64KB 淘汰出 PageCache 前，进程读取到它了，收益就⾮常⼤。

​	所以，PageCache 的优点主要是两个：

- 缓存最近被访问的数据；
- 预读功能；

​    这两个做法，将⼤⼤提⾼读写磁盘的性能。

​	但是，在传输⼤⽂件（GB 级别的⽂件）的时候，PageCache 会不起作⽤，那就⽩⽩浪费 DMA 多做的⼀次数据拷⻉，造成性能的降低，即使使⽤了 PageCache 的零拷⻉也会损失性能

​	这是因为如果你有很多 GB 级别⽂件需要传输，每当⽤户访问这些⼤⽂件的时候，内核就会把它们载⼊PageCache 中，于是 PageCache空间很快被这些⼤⽂件占满。另外，由于⽂件太⼤，可能某些部分的⽂件数据被再次访问的概率⽐较低，这样就会带来 2 个问题：

- PageCache 由于⻓时间被⼤⽂件占据，其他「热点」的⼩⽂件可能就⽆法充分使⽤到 PageCache，于是这样磁盘读写的性能就会下降了；
- PageCache 中的⼤⽂件数据，由于没有享受到缓存带来的好处，但却耗费 DMA 多拷⻉到PageCache ⼀次；

​     所以，针对⼤⽂件的传输，不应该使⽤ PageCache，也就是说不应该使⽤零拷⻉技术，因为可能由于PageCache 被⼤⽂件占据，⽽导致「热点」⼩⽂件⽆法利⽤到 PageCache，这样在⾼并发的环境下，会带来严重的性能问题。

***

##### ⼤⽂件传输⽤什么⽅式实现？

​	可以采用异步IO的实现方式(需要注意的是，磁盘控制缓冲区和PageCache是不一样的，PageCache属于内核缓冲区的)

<img src="[OS]网络-异步IO读.png" alt="[OS]网络-异步IO读" style="zoom:50%;" />

​	它把读操作分为两部分：

- 前半部分，内核向磁盘发起读请求，但是可以不等待数据就位就可以返回，于是进程此时可以处理其他任务；
- 后半部分，当内核将磁盘中的数据拷⻉到进程缓冲区后，进程将接收到内核的通知，再去处理数据；

​    ⽽且，我们可以发现，异步 I/O 并没有涉及到 PageCache，所以使⽤异步 I/O 就意味着要绕开PageCache。

​	绕开 PageCache 的 I/O 叫直接 I/O，使⽤ PageCache 的 I/O 则叫缓存 I/O。通常，对于磁盘，异步 I/O 只⽀持直接 I/O。

​	前⾯也提到，⼤⽂件的传输不应该使⽤ PageCache，因为可能由于 PageCache 被⼤⽂件占据，⽽导致「热点」⼩⽂件⽆法利⽤到 PageCache。

​	于是，在⾼并发的场景下，针对⼤⽂件的传输的⽅式，应该使⽤「异步 I/O + 直接 I/O」来替代零拷⻉技术。

***

##### select和poll的工作过程

​	select 实现多路复⽤的⽅式是，将已连接的 Socket 都放到⼀个⽂件描述符集合，然后调⽤ select函数将⽂件描述符集合拷⻉到内核⾥，让内核来检查是否有⽹络事件产⽣，检查的⽅式很粗暴，就是通过遍历⽂件描述符集合的⽅式，当检查到有事件产⽣后，将此 Socket 标记为可读或可写， 接着再把整个⽂件描述符集合拷⻉回⽤户态⾥，然后⽤户态还需要再通过遍历的⽅法找到可读或可写的 Socket，然后再对其处理。

​	所以，对于 select 这种⽅式，需要进⾏ 2 次「遍历」⽂件描述符集合，⼀次是在内核态⾥，⼀个次是在⽤户态⾥ ，⽽且还会发⽣ 2 次「拷⻉」⽂件描述符集合，先从⽤户空间传⼊内核空间，由内核修改后，再传出到⽤户空间中。

​	select 使⽤固定⻓度的 BitsMap，表示⽂件描述符集合，⽽且所⽀持的⽂件描述符的个数是有限制的，在Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最⼤值为 1024 ，只能监听 0~1023 的⽂件描述符。

​	poll 不再⽤ BitsMap 来存储所关注的⽂件描述符，取⽽代之⽤动态数组，以链表形式来组织，突破了select 的⽂件描述符个数限制，当然还会受到系统⽂件描述符限制。

​	但是 poll 和 select 并没有太⼤的本质区别，都是使⽤「线性结构」存储进程关注的 Socket 集合，因此都需要遍历⽂件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，⽽且也需要在⽤户态与内核态之间拷⻉⽂件描述符集合，这种⽅式随着并发数上来，性能的损耗会呈指数级增⻓。

***

##### epoll 的工作过程

​	epoll 通过两个⽅⾯，很好解决了 select/poll 的问题。

​	1、epoll 在内核⾥使⽤红⿊树来跟踪进程所有待检测的⽂件描述字，**把需要监控的 socket 通过epoll_ctl() 函数加⼊内核中的红⿊树⾥(将socket上树)**，红⿊树是个⾼效的数据结构，增删查⼀般时间复杂度是O(logn) ，通过对这棵⿊红树进⾏操作，这样就不需要像 select/poll 每次操作时都传⼊整个 socket 集合，只需要传⼊⼀个待检测的 socket，减少了内核和⽤户空间⼤量的数据拷⻉和内存分配。

​	2、epoll 使⽤事件驱动的机制，内核⾥维护了⼀个链表来记录就绪事件，当某个 socket 有事件发⽣时，通过回调函数内核会将其加⼊到这个就绪事件列表中，**当⽤户调⽤ epoll_wait() 函数时，只会返回有事件发⽣的⽂件描述符的个数**，不需要像 select/poll 那样轮询扫描整个 socket 集合，⼤⼤提⾼了检测的效率。

​	epoll 的⽅式即使监听的 Socket 数量越多的时候，效率不会⼤幅度降低，能够同时监听的 Socket 的数⽬也⾮常的多了，上限就为系统定义的进程打开的最⼤⽂件描述符个数。因⽽，epoll 被称为解决 C10K 问题的利器。

***

##### epoll 的两种触发模式

​	epoll ⽀持两种事件触发模式，分别是⽔平触发（level-triggered，LT）和边缘触发（edge-triggered，ET）

- 使⽤⽔平触发模式时，当被监控的 Socket 上有可读事件发⽣时，服务器端不断地从 epoll_wait 中苏醒(所以我们在代码中，只需要每次 epoll_wait 的时候进行 read 即可)，直到内核缓冲区数据被 read 函数读完才结束，⽬的是告诉我们有数据需要读取；
- 使⽤边缘触发模式时，当被监控的 Socket 描述符上有可读事件发⽣时，服务器端只会从 epoll_wait中苏醒⼀次，即使进程没有调⽤ read 函数从内核读取数据，也依然只苏醒⼀次，因此我们程序要保证⼀次性将内核缓冲区的数据读取完(所以我们代码中，要用 while 死循环调用 read，直到把数据都读完了)；

​    使⽤边缘触发模式的时候，I/O 事件发⽣时只会通知⼀次，我们会循环从⽂件描述符读写数据，那么如果⽂件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那⾥，程序就没办法继续往下执⾏。**所以，边缘触发模式⼀般和⾮阻塞 I/O 搭配使⽤**，程序会⼀直执⾏ I/O 操作，直到系统调⽤（如 read 和write ）返回错误，错误类型为 EAGAIN 或 EWOULDBLOCK 。

​	⼀般来说，边缘触发的效率⽐⽔平触发的效率要⾼，因为边缘触发可以减少 epoll_wait 的系统调⽤次数。select/poll 只有⽔平触发模式，epoll 默认的触发模式是⽔平触发，但是可以根据应⽤场景设置为边缘触发模式。

***

##### 网络IO模型的演进：由多进程/线程到IO多路复用

​	如果要让服务器服务多个客户端，那么最直接的⽅式就是为每⼀条连接创建线程。

​	处理完业务逻辑后，随着连接关闭后线程也同样要销毁了，但是这样不停地创建和销毁线程，不仅会带来性能开销，也会造成浪费资源，⽽且如果要连接⼏万条连接，创建⼏万个线程去应对也是不现实的。

​	要怎么解决这个问题呢？我们可以使⽤「资源复⽤」的⽅式。

​	也就是不⽤再为每个连接创建线程，⽽是创建⼀个「线程池」，将连接分配给线程，然后⼀个线程可以处理多个连接的业务。

​	不过，这样⼜引来⼀个新的问题，线程怎样才能⾼效地处理多个连接的业务？

​	当⼀个连接对应⼀个线程时，线程⼀般采⽤「read -> 业务处理 -> send」的处理流程，如果当前连接没有数据可读，那么线程会阻塞在 read 操作上（ socket 默认情况是阻塞 I/O），不过这种阻塞⽅式并不影响其他线程。

​	但是引⼊了线程池，那么⼀个线程要处理多个连接的业务，线程在处理某个连接的 read 操作时，如果遇到没有数据可读，就会发⽣阻塞，那么线程就没办法继续处理其他连接的业务。

​	解决这⼀个问题，最简单的⽅式就是将 socket 改成⾮阻塞，然后线程不断地轮询调⽤ read 操作来判断是否有数据，这种⽅式虽然该能够解决阻塞的问题，但是解决的⽅式⽐较粗暴，因为轮询是要消耗 CPU 的，⽽且随着⼀个 线程处理的连接越多，轮询的效率就会越低。

​	**上⾯的问题在于，线程并不知道当前连接是否有数据可读，从⽽需要每次通过 read 去试探。**

​	那有没有办法在只有当连接上有数据的时候，线程才去发起读请求呢？答案是有的，实现这⼀技术的就是I/O 多路复⽤。

​	I/O 多路复⽤技术会⽤⼀个系统调⽤函数来监听我们所有关⼼的连接，也就说可以在⼀个监控线程⾥⾯监控很多的连接。

​	select/poll/epoll 就是内核提供给⽤户态的多路复⽤系统调⽤，线程可以通过⼀个系统调⽤函数从内核中获取多个事件。

***

##### 当下开源软件能做到⽹络⾼性能的原因就是 I/O 多路复⽤吗？

​	是的，基本是基于 I/O 多路复⽤，⽤过 I/O 多路复⽤接⼝写⽹络程序的同学，肯定知道是⾯向过程的⽅式写代码的，这样的开发的效率不⾼。于是，⼤佬们基于⾯向对象的思想，对 I/O 多路复⽤作了⼀层封装，让使⽤者不⽤考虑底层⽹络 API 的细节，只需要关注应⽤代码的编写。这种模式就是：**Reactor 模式。**

​	Reactor 翻译过来的意思是「反应堆」，反应指的是「对事件反应」，也就是来了⼀个事件，Reactor 就有相对应的反应/响应。

​	事实上，Reactor 模式也叫 Dispatcher 模式，即 I/O 多路复⽤监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程 / 线程。Reactor 模式主要由 Reactor 和处理资源池这两个核⼼部分组成，它俩负责的事情如下：

- Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件；
- 处理资源池负责处理事件，如 read -> 业务逻辑 -> send；

​    Reactor 模式是灵活多变的，可以应对不同的业务场景，灵活在于：

- Reactor 的数量可以只有⼀个，也可以有多个；
- 处理资源池可以是单个进程 / 线程，也可以是多个进程 /线程；

​    在实际应用过程中，有3个方案比较经典，且都有应用在实际的项目中

- 单 Reactor 单进程 / 线程；
- 单 Reactor 多线程 / 进程
- 多 Reactor 多进程 / 线程；

***

##### 单 Reactor 单进程 / 线程的实现

​	「单 Reactor 单进程」的⽅案示意图如下：

<img src="[OS]网络-IO模型-单Reactor单进程.png" alt="[OS]网络-IO模型-单Reactor单进程" style="zoom:50%;" />

​	可以看到进程⾥有 Reactor、Acceptor、Handler 这三个对象：

- Reactor 对象的作⽤是监听和分发事件；
- Acceptor 对象的作⽤是获取连接；
- Handler 对象的作⽤是处理业务；

​    对象⾥的 select、accept、read、send 是系统调⽤函数，dispatch 和 「业务处理」是需要完成的操作，其中 dispatch 是分发事件操作。

​	接下来，介绍下「单 Reactor 单进程」这个⽅案：

- Reactor 对象通过 select （IO 多路复⽤接⼝） 监听事件，收到事件后通过 dispatch 进⾏分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
- 如果是连接建⽴的事件，则交由 Acceptor 对象进⾏处理，Acceptor 对象会通过 accept ⽅法 获取连接，并创建⼀个 Handler 对象来处理后续的响应事件；
- 如果不是连接建⽴事件， 则交由当前连接对应的 Handler 对象来进⾏响应；Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

​    单 Reactor 单进程的⽅案因为全部⼯作都在同⼀个进程内完成，所以实现起来⽐较简单，不需要考虑进程间通信，也不⽤担⼼多进程竞争。但是，这种⽅案存在 2 个缺点：

- 第⼀个缺点，因为只有⼀个进程，⽆法充分利⽤ 多核 CPU 的性能；
- 第⼆个缺点，Handler 对象在业务处理时，整个进程是⽆法处理其他连接的事件的，如果业务处理耗时⽐较⻓，那么就造成响应的延迟；

​    所以，单 Reactor 单进程的⽅案不适⽤计算机密集型的场景，只适⽤于业务处理⾮常快速的场景。Redis 是由 C 语⾔实现的，它采⽤的正是「单 Reactor 单进程」的⽅案，因为 Redis 业务处理主要是在内存中完成，操作的速度是很快的，性能瓶颈不在 CPU 上，所以 Redis 对于命令的处理是单进程的⽅案。

*****

##### 单 Reactor 多线程 / 多进程的实现

​	如果要克服「单 Reactor 单线程 / 进程」⽅案的缺点，那么就需要引⼊多线程 / 多进程，这样就产⽣了单Reactor 多线程 / 多进程的⽅案。其示意图如下：

<img src="[OS]网络-IO模型-单Reactor多进程.png" alt="[OS]网络-IO模型-单Reactor多进程" style="zoom:40%;" />

​	详细说⼀下这个⽅案：

- Reactor 对象通过 select （IO 多路复⽤接⼝） 监听事件，收到事件后通过 dispatch 进⾏分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
- 如果是连接建⽴的事件，则交由 Acceptor 对象进⾏处理，Acceptor 对象会通过 accept ⽅法 获取连接，并创建⼀个 Handler 对象来处理后续的响应事件；	
- 如果不是连接建⽴事件， 则交由当前连接对应的 Handler 对象来进⾏响应；

​    上⾯的三个步骤和单 Reactor 单线程⽅案是⼀样的，接下来的步骤就开始不⼀样了：

- Handler 对象不再负责业务处理，只负责数据的接收和发送，Handler 对象通过 read 读取到数据后，会将数据发给⼦线程⾥的 Processor 对象进⾏业务处理；
- ⼦线程⾥的 Processor 对象就进⾏业务处理，处理完后，将结果发给主线程中的 Handler 对象，接着由 Handler 通过 send ⽅法将响应结果发送给 client；

   单 Reator 多线程的⽅案优势在于能够充分利⽤多核 CPU 的能，那既然引⼊多线程，那么⾃然就带来了多线程竞争资源的问题。

​	例如，⼦线程完成业务处理后，要把结果传递给主线程的 Reactor 进⾏发送，这⾥涉及共享数据的竞争。要避免多线程由于竞争共享资源⽽导致数据错乱的问题，就需要在操作共享资源前加上互斥锁，以保证任意时间⾥只有⼀个线程在操作共享资源，待该线程操作完释放互斥锁后，其他线程才有机会操作共享数据。

​	事实上，单 Reactor 多进程相⽐单 Reactor 多线程实现起来很麻烦，主要因为要考虑⼦进程 <-> ⽗进程的双向通信，并且⽗进程还得知道⼦进程要将数据发送给哪个客户端。

​	⽽多线程间可以共享数据，虽然要额外考虑并发问题，但是这远⽐进程间通信的复杂度低得多，因此实际应⽤中也看不到单 Reactor 多进程的模式。

​	另外，「单 Reactor」的模式还有个问题，因为⼀个 Reactor 对象承担所有事件的监听和响应，⽽且只在主线程中运⾏，在⾯对瞬间⾼并发的场景时，容易成为性能的瓶颈的地⽅。

***

##### 多 Reactor 多进程 / 线程的实现

​	要解决「单 Reactor」的问题，就是将「单 Reactor」实现成「多 Reactor」，这样就产⽣了第 多Reactor 多进程 / 线程的⽅案。方案实现如下：

<img src="[OS]网络-IO模型-多Reactor多进程.png" alt="[OS]网络-IO模型-多Reactor多进程" style="zoom:40%;" />

​	⽅案详细说明如下：

- 主线程中的 MainReactor 对象通过 select 监控连接建⽴事件，收到事件后通过 Acceptor 对象中的accept 获取连接，将新的连接分配给某个⼦线程；
- ⼦线程中的 SubReactor 对象将 MainReactor 对象分配的连接加⼊ select 继续进⾏监听，并创建⼀个Handler ⽤于处理连接的响应事件。
- 如果有新的事件发⽣时，SubReactor 对象会调⽤当前连接对应的 Handler 对象来进⾏响应。
- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

​    多 Reactor 多线程的⽅案虽然看起来复杂的，但是实际实现时⽐单 Reactor 多线程的⽅案要简单的多，原因如下：

- 主线程和⼦线程分⼯明确，主线程只负责接收新连接，⼦线程负责完成后续的业务处理。
- 主线程和⼦线程的交互很简单，主线程只需要把新连接传给⼦线程，⼦线程⽆须返回数据，直接就可以在⼦线程将处理结果发送给客户端。

​    ⼤名鼎鼎的两个开源软件 Netty 和 Memcache 都采⽤了「多 Reactor 多线程」的⽅案。

​	采⽤了「多 Reactor 多进程」⽅案的开源软件是 Nginx，不过⽅案与标准的多 Reactor 多进程有些差异。具体差异表现在主进程中仅仅⽤来初始化 socket，并没有创建 mainReactor 来 accept 连接，⽽是由⼦进程的 Reactor 来 accept 连接，通过锁来控制⼀次只有⼀个⼦进程进⾏ accept（防⽌出现惊群现象），⼦进程 accept 新连接后就放到⾃⼰的 Reactor 进⾏处理，不会再分配给其他⼦进程。

***

##### Reactor 和 Proactor 的区别

- **Reactor 是⾮阻塞同步⽹络模式，感知的是就绪可读写事件。**在每次感知到有事件发⽣（⽐如可读就绪事件）后，就需要应⽤进程主动调⽤ read ⽅法来完成数据的读取，也就是要应⽤进程主动将socket 接收缓存中的数据读到应⽤进程内存中，这个过程是同步的，读取完数据后应⽤进程才能处理数据。
- **Proactor 是异步⽹络模式，感知的是已完成的读写事件。**在发起异步读写请求时，需要传⼊数据缓冲区的地址（⽤来存放结果数据）等信息，这样系统内核才可以⾃动帮我们把数据的读写⼯作完成，这⾥的读写⼯作全程由操作系统来做，并不需要像 Reactor 那样还需要应⽤进程主动发起 read/write来读写数据，操作系统完成读写⼯作后，就会通知应⽤进程直接处理数据。

​    因此，**Reactor 可以理解为「来了事件操作系统通知应⽤进程，让应⽤进程来处理」**，⽽**Proactor可以理解为「来了事件操作系统来处理，处理完再通知应⽤进程」**。这⾥的「事件」就是有新连接、有数据可读、有数据可写的这些 I/O 事件这⾥的「处理」包含从驱动读取到内核以及从内核读取到⽤户空间。

​	举个实际⽣活中的例⼦，Reactor 模式就是快递员在楼下，给你打电话告诉你快递到你家⼩区了，你需要⾃⼰下楼来拿快递。⽽在 Proactor 模式下，快递员直接将快递送到你家⻔⼝，然后通知你。

​	无论是 Reactor，还是 Proactor，都是⼀种基于「事件分发」的⽹络编程模式，区别在于 **Reactor 模式是基于「待完成」的 I/O 事件**，**⽽ Proactor 模式则是基于「已完成」的 I/O 事件**。

***

##### Proactor 的实现

方案实例如下

<img src="[OS]网络-IO模型-Proactor模型.png" alt="[OS]网络-IO模型-Proactor模型" style="zoom:50%;" />

​	介绍⼀下 Proactor 模式的⼯作流程：

- Proactor Initiator 负责创建 Proactor 和 Handler 对象，并将 Proactor 和 Handler 都通过Asynchronous Operation Processor 注册到内核；
- Asynchronous Operation Processor 负责处理注册请求，并处理 I/O 操作；
- Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor；
- Proactor 根据不同的事件类型回调不同的 Handler 进⾏业务处理；
- Handler 完成业务处理；

​    可惜的是，在 Linux 下的异步 I/O 是不完善的，aio 系列函数是由 POSIX 定义的异步操作接⼝，不是真正的操作系统级别⽀持的，⽽是在⽤户空间模拟出来的异步，并且仅仅⽀持基于本地⽂件的 aio 异步操作，⽹络编程中的 socket 是不⽀持的，这也使得基于 Linux 的⾼性能⽹络程序都是使⽤ Reactor ⽅案。
